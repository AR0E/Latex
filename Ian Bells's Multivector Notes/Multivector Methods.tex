\documentclass[a4paper]{book}


\usepackage{mlmodern}			% Usa a fonte Latin Modern			
\usepackage[T1]{fontenc}		% seleção de códigos de fonte.
\usepackage[utf8]{inputenc}		% determina a codificação utiizada (conversão automática dos acentos)
\usepackage[brazil]{babel}
\usepackage{hyperref}  			% controla a formação do índice
%\usepackage{parskip}			% espaçamento entre os parágrafos
\usepackage{nomencl} 			% Lista de simbolos
\usepackage{microtype} 			% para melhorias de justificação
\usepackage{booktabs}			% para tabelas
\setlength{\belowcaptionskip}{6pt} % espaçamento depois do título das tabelas
%\setlength{\abovecaptionskip}{6pt}

\hypersetup{
		colorlinks=true,       		% false: boxed links; true: colored links
		linkcolor=blue,        		% color of internal links
		citecolor=blue,        		% color of links to bibliography
		filecolor=magenta,     		% color of file links
		urlcolor=blue}

\IfFileExists{html.sty}
{\usepackage{html}}
{\usepackage{comment}
 \excludecomment{htmlonly}
 \excludecomment{rawhtml}
 \includecomment{latexonly}
 \newcommand{\html}[1]{}
 \newcommand{\latex}[1]{##1}
 \ifx\undefined\hyperref
  \ifx\pdfoutput\undefined \let\pdfunknown\relax
   \let\htmlATnew=\newcommand
  \else
   \ifx\pdfoutput\relax \let\pdfunknown\relax
    \usepackage{hyperref}\let\htmlATnew=\renewcommand
   \else
    \usepackage{hyperref}\let\htmlATnew=\newcommand
 \fi
  \fi
 \else
  \let\htmlATnew=\renewcommand
 \fi
 \ifx\pdfunknown\relax
  \htmlATnew{\htmladdnormallink}[2]{##1}
 \else
  \def\htmladdnormallink##1##2{\href{##2}{##1}}
 \fi
 \long\def\latexhtml##1##2{##1}}

% latex2html nao suporta \ifthenelse...
\usepackage[num,abnt-verbatim-entry=yes]{abntex2cite}
%overcite
%\citebrackets{}

\newcommand{\OKs}{173}
\newcommand{\quaseOKs}{12}
\newcommand{\nadaOK}{3}

\def\Versao$#1 #2${#2}
\def\Data$#1 #2 #3${#2}
%\newcommand{\bibtextitlecommand}[2]{``#2''}

\usepackage{color}
\definecolor{thered}{rgb}{0.65,0.04,0.07}
\definecolor{thegreen}{rgb}{0.06,0.44,0.08}
\definecolor{thegrey}{gray}{0.5}
\definecolor{theshade}{rgb}{1,1,0.97}
\definecolor{theframe}{gray}{0.6}

\IfFileExists{listings.sty}{
  \usepackage{listings}
\lstset{%
	language=[LaTeX]TeX,
	columns=flexible,
	basicstyle=\ttfamily\small,
	backgroundcolor=\color{theshade},
	frame=single,
	tabsize=2,
	rulecolor=\color{theframe},
	title=\lstname,
	escapeinside={\%*}{*)},
	breaklines=true,
	commentstyle=\color{thegrey},
	keywords=[0]{\fichacatalografica,\errata,\folhadeaprovacao,\dedicatoria,\agradecimentos,\epigrafe,\resumo,\siglas,\simbolos,\citacao,\alineas,\subalineas,\incisos},
	keywordstyle=[0]\color{thered},
	keywords=[1]{},
	keywordstyle=[1]\color{thegreen},
	breakatwhitespace=true,
	alsoother={0123456789_},
	inputencoding=utf8,
	extendedchars=true,
	literate={á}{{\'a}}1 {ã}{{\~a}}1 {é}{{\'e}}1 {è}{{\`{e}}}1 {ê}{{\^{e}}}1 {ë}{{\¨{e}}}1 {É}{{\'{E}}}1 {Ê}{{\^{E}}}1 {û}{{\^{u}}}1 {ú}{{\'{u}}}1 {â}{{\^{a}}}1 {à}{{\`{a}}}1 {á}{{\'{a}}}1 {ã}{{\~{a}}}1 {Á}{{\'{A}}}1 {Â}{{\^{A}}}1 {Ã}{{\~{A}}}1 {ç}{{\c{c}}}1 {Ç}{{\c{C}}}1 {õ}{{\~{o}}}1 {ó}{{\'{o}}}1 {ô}{{\^{o}}}1 {Õ}{{\~{O}}}1 {Ó}{{\'{O}}}1 {Ô}{{\^{O}}}1 {î}{{\^{i}}}1 {Î}{{\^{I}}}1 {í}{{\'{i}}}1 {Í}{{\~{Í}}}1,
}
\let\verbatim\relax
 	\lstnewenvironment{verbatim}[1][]{\lstset{##1}}{}
}

\usepackage[T1]{fontenc}		
\usepackage[utf8]{inputenc}		
\usepackage{indentfirst}	

\usepackage{listings}
\usepackage[table,xcdraw]{xcolor}			
\usepackage{graphicx}	
\usepackage{physics}

\usepackage{microtype}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage[makeroom]{cancel}

\usepackage{siunitx}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage{nicematrix} %Matrizes bonitas
\usepackage{xkcdcolors} %Cores do xkcd
\usepackage{tikz, tcolorbox, bclogo} %Desenhos e afins
\usepackage{empheq} %Caixas ao redor de equações em um align

\usepackage{float}
\usepackage{caption}
\usepackage{cleveref}
\usepackage{epigraph} 

\usepackage{tabularx}
\usepackage{array}
\usepackage{hyperref}
\usepackage[Sonny]{fncychap}


\numberwithin{equation}{chapter}

\definecolor{blue}{RGB}{41,5,195}

\frenchspacing

\begin{document}

\newcommand{\titulo}{\textbf{Ian Bell's notes}\\ \textbf{I : Multivector Methods}}
\newcommand{\abnTeX}{abn\TeX}

%begin{latexonly}
\title{\titulo}

\author{
Artur R. B. Boyago
}
\date{\today}

\maketitle

\tableofcontents

\section{Preface}

All of the content here originally comes from \href{http://www.iancgbell.clara.net/maths/geoalg.htm}{Ian Bell's
stunning 1999 page}. However,
due to the outdated graphic design and possible depredation of the site in question, there is a 
risk of the material eventually becoming unreadable.

\vspace{\baselineskip}

Further, I'd like to add lots of personal notes on this kind of mathematics from  studies
in the matter in order to enhance the original material, so there will be undisclosed
differences, as well as interjections in \textcolor{gray}{gray color}.

The book is formatted in the same order as the original
with all content reproduced in modern \LaTeX format. Many thanks, Artur.

\chapter{Multivectors}

\section{Introduction}

Geometric algebra provides a practical alternative to conventional 3D vector methods which extends far more 
readily to higher dimensions. It also provides a coordinate independant symbolic geometry (an \emph{'algebra of 
directions'} \cite{grassman1}) extendable into a \textbf{geometric calculus} \cite{hestenes1} of profound relevance to areas as diverse as quantum physics and computer vision.

\textcolor{gray}{For a more detailed historical outline, do check out 
this document and this write-up in my \href{https://ar0e.github.io}{webpage}}.

\vspace{\baselineskip}

The purpose of this work is to provide a concise but comprehensive introduction and 
broad reference for 
geometric algebra for those interested in it as a powerful computational and theoretical
resource that spans
and unifies a diverse range of fields.

\vspace{\baselineskip}

Fuller, more formal mathematical treatments exist elsewhere and this 
document can serve as a primer for tackling such works. It assumes familiarity with 
'conventional' 3D 
constructs such as vectors and matrices and such basic mathematical functions as 
$\cos(\theta)$ and ex.  Mathematical notations used are defined in the glossary.

Multivectors, the 'elements' of Geometric Algbera, are a generalisation of traditional vectors that provide a 
far richer mathematical structure than vectors alone. Many programmers will have encountered particular 
multivectors before in the form of complex numbers and quaternions, and the closest analogy for the 
generalisation of vectors to multivectors is perhaps complex numbers as a generalisation of real numbers.

\vspace{\baselineskip}

Regarding real numbers as a special case of the more general 'class' or 'field' of complex numbers allows 
logarithms of negative numbers and additional functions such as 'complex conjugation'. By generalising 
vectors to a particular subset of multivectors not only can vectors be multiplied and divided by eachother,
but we obtain a multitude of useful conjugations and bilinear products and can usefully define logarithmic 
and trigonometric functions of them. Since particular $n+2$ dimensional multivectors can represent arbitary 
$n$-dimesional lines, circles, planes, and spheres we can, for example, speak of taking the log of a sphere. 
Geometric objects become algebra elements ameniable to both purely symbolic and computational (numerical) 
manipulation.

\vspace{\baselineskip}

The notion of multiplying two 3D vectors together is familiar to programmers as the so called vector cross 
product $\mathbf{a}\times \mathbf{b}$ being a vector of direction perpendicular to vectors a and b with magnitude $|\mathbf{a}||\mathbf{b}| \sin(\theta)$ 
where $\theta$ 
is the angle subtended by $\mathbf{a}$ and $\mathbf{b}$. However, this only works in 3D. The fundamental essence of geometric 
algebra is the geometric vector product $\mathbf{ab = a\wedge b +
a \cdot b}$ where $\cdot$ is the traditional scalar-valued vector dot 
product and the \emph{'2-blade'} $\mathbf{a\wedge b}$ is 
the \emph{\textbf{wedge}} or \emph{outer product} described later. Everything else follows from this. 

\vspace{\baselineskip}

This section describes what multivectors are mathematically and lists the many operations and products that 
can be usefully applied to them in considerable detail. Of necessity some of this material is mathematically 
intensive and the reader is is encouraged to 'skim' rather than absorbing every product, conjugation,
normalisation technique and logarithmic computation strategum on their first pass. The 'point' of 
multivectors is what you can do with them, and this is addressed in the later sections once the basics of the 
symbolic and computational manipulations are covered.

\vspace{\baselineskip}

When first encountering multivectors they can seem bewildering, with a plethora of products and an overload
of operators provided for their manipulation. But familiarity breeds respect. Given that multivectors are the
'language' of (dynamic) geometry and by implication of nature itself, a half-dozen new symbols does not seem 
excessive.


\vspace{\baselineskip}
\begin{itemize}
    \item In the \textbf{Multivectors Programming} chapter we describe how to impliment multivectors of both low and high dimension $n$ in C/C++.

    \item In the \textbf{Multivectors as Geometric Objects} we will see some of the applications of 
    multivectors in the elegant representation and manipulation of N-dimensional spheres, planes, lines
    and conics.

    \item In the \textbf{Multivectors as Geometric Objects} we will see some of the applications of multivectors in the elegant representation and manipulation of $n$-dimensional spheres, planes, lines and conics.
    
    \item In the \textbf{Multivectors as Transformations} we will see how multivectors can also be used to transform, distort, displace, and morph such geometric constructs.
    
    \item In \textbf{Multivector Arcana} we cover some more esoteric mathematical aspects of multivectors of less general interest. In later sections we cover \emph{Multivector Calculus} and the uses of multivectors in physics, in  particular Relativity and Quantum Mechanics.
\end{itemize}

   
\vspace{\baselineskip}

The book favours the left contraction $\mathbf{a}\lrcorner\mathbf{b}$
over the symmetric Hestenes $\mathbf{a}\cdot\mathbf{b}$ product as it is
arguably more fundamental and has more functional advantages.

\section{Multivectors}

\epigraph{And therefore in geometry (which is the only science that it hath pleased God hitherto to bestow on mankind), men begin at settling the significations of their words; which settling of significations, they call definitions, and place them in the beginning of their reckoning}{\textit{Thomas Hobbes, Leviathan  }}

Although a notable advantage of geometric algebra is coordinate independance, we will 
initially take an orthonormal coordinate (basis) based approach here since this is likely 
to be the most practicable in a programming context.
Geometric algebra is essentially a set of arithmetical techniques for manipulating 
$n$-dimensional vectors and to see how to properly multiply (and divide!) vectors we must
first generalise the concept of vectors and scalars.

\vspace{\baselineskip}
 
Given $k-$linearly independant $n$-dimensional vectors 
${\mathbf{e}_1,\mathbf{e}_2, \cdots ,\mathbf{e}_k}$ from a vector 
space $V^n$, 
their \textbf{\emph{outer product}} $\mathbf{e}_k = \mathbf{e}_1
\wedge \mathbf{e}_2 \cdots \wedge \mathbf{e}_k$, (not to be confused 
with the 3D vector "cross" ($\times$) product)  is known as a 
\emph{blade} of \emph{grade} (aka. \emph{step} or \emph{degree}) $k$,
or a $k-$blade. 

\vspace{\baselineskip}

We are interested initially in $V^n=\mathbb{R}^n$, the space of real 
coordinate $n$-D vectors, but will refer to $V^n$ to emphasise 
applicability to alternate (eg. non-Euclidean) spaces which are of 
interest to us, particularly with regard to relativistic physics. By 
\emph{"linearly independant"} we here mean that no one of the 
${\mathbf{e}_1,\mathbf{e}_2, \cdots ,\mathbf{e}_k}$ can be expressed 
as a real-weighted sum of the others.

\vspace{\baselineskip}

The fundamental rules for $\wedge$ (the \textbf{exterior algebra}) 
are:

\begin{tcolorbox}[colback=white, colframe = purple!60!black, title=\textbf{Exterior Product Rules} ]

\begin{itemize}
    \item \textbf{Antisymmetry:} $\mathbf{a}\wedge \mathbf{b} = -\mathbf{b}\wedge \mathbf{q}$
    
    \item \textbf{Linearity:} $\mathbf{a}\wedge(\textbf{b}+\mathbf{c}) = \mathbf{a}\wedge\mathbf{b}+\mathbf{a}+\mathbf{c}$
    
    \item \textbf{Associativity:} $\mathbf{a}\wedge(\mathbf{b}\wedge \mathbf{c})=(\mathbf{a}\wedge\mathbf{b})\wedge\mathbf{c}$
\end{itemize}

\end{tcolorbox} 

\textcolor{gray}{Such a construction is known as the \emph{exterior algebra}, a subset
of geometric algebra that only uses this product and has no reference to a given metric.
This is extremely useful as one can get far-reaching topological results in \emph{exterior
calculus.}}

A $k$-blade $\mathbf{A}_k$ can be thought of as representing an orientated and scaled 
$k$-dimensional subspace of $V^n$, one in which all the vectors satisfy 
$\mathbf{A\wedge A_k} = 0$. 
We say that $\mathbf{A_1, \cdots, A_k}$ are a \emph{$k$-frame} for this subspace. 

\vspace{\baselineskip}

A linear \emph{"weighted additive"} combination of $k$-blades is known as a $k$-vector. 
An example 4D 3-vector is 
$ 2\mathbf{e_1\wedge e_2\wedge e_3} + (\sqrt{7})\mathbf{e_3 \wedge e_1 \wedge e_2}$.

\begin{itemize}
    \item A 0-vector is a 0-blade is a \emph{scalar}. We refer to a $k$-blade as proper if $k\ge1$.
    \item A 1-vector is a 1-blade is a conventional vector.
    \item A 2-vector (aka. $bivector$) is a sum of scaled 2-blades and need not "reduce" to a single 2-blade for $n>3$. 2-blades can be considered geometrically as directed areas (ie. a plane and a signed scalar).
    \item For $n\le3$, any $k$-vector is a $k$-blade. This is true only for $k<2$ when $n > 3$.
\end{itemize}

\textcolor{gray}{The pictorial interpretation would be that each $k$-vector is a
$k-$dimensional volume element within its own vector subspace. Here is a picture}:

\vspace{\baselineskip}

\noindent
\begin{minipage}{\linewidth}
\makebox[\linewidth]{
  \includegraphics[keepaspectratio=true,scale=0.7]{images/gaforag1.png}
  }
\captionof{figure}{
The main objects in geometric algebra for three dimensions.
}\label{fig:ga1}
\end{minipage}

\vspace{\baselineskip}

This makes $n=3$ a fundamentally simpler case than $n\ge4$ to the 
extent that "geometric intuitions" founded in 3D can be actively misleading for $n\ge4$.
Because of this, the reader is here advised to initally consider multivectors as algebraic,
rather than geometric, entities to be manipulated symbolically 
and numerically using grammar rules rather than pertaining to geometric constructions.

\vspace{\baselineskip}

Consider the set $\mathcal{R}^n$ (aka. $\mathcal{G}(n)$ and 
$\mathcal{Cl}(n)$ in the literature) of all linear combinations (with real "coefficients" or 
"wieghtings") of $k$-vectors for $0\lek\le n$, our 1-vectors being taken from the $n$-dimensional vector space $\mathbb{R}^n$. Clearly $\mathbb{R}^n \subset \mathcal{R}_n$ 
(or, more properly, is represented within $\mathcal{R}_n$) 
and infact $\mathcal{R}_n$ has dimension $\Sigma_{k=0}^n C_k^n = 2^k$, 
there being $C^n_k \equiv n!(n-k)!^{-1}k!^{-1}$ distinct $k$-blades in $N$-dimensions.

\vspace{\baselineskip}

We refer to the elements of $\mathcal{R}_n$ as \textbf{\emph{multivectors}}.
An example $\mathcal{R}_3$ multivector, albeit one unlikely to arise in practice, is:

\begin{align*}
    3+\mathbf{e_1}-4\mathbf{e_2} + \frac{1}{2} \mathbf{e_1}\wedge \mathbf{e_2}
    +(\sqrt{7})\mathbf{e_2}\wedge\mathbf{e_3} + \pi \mathbf{e_1}\wedge \mathbf{e_2} \wedge
    \mathbf{e_3}
\end{align*}

\vspace{\baselineskip}

We can thus think of a general $n$-D multivector as an arbitary real-weighted combination of $2n$ distinct basis blades. Mathematically, one can take the "blade coefficients" from any 
field $\mathbf{F}$ or "number space". It is the utilisation of $\mathbb{R}$, the real numbers,
- essentially identifying the blade "coefficients" or "coordinates" with "scalars" (0-vectors)
- that distinguishes "geometric algebra" from more general Clifford algebras of only passing 
concern here. If we allow "complex number" blade coeeficients, for example,  we obtain a space 
$\mathcal{C}_n/\mathcal{G}(\mathbb{C}^n)$ of dimension $2n+1$.
    
    \vspace{\baselineskip}
    
 Multivectors can thus be represented with $2n$ dimensional 1-vectors, with respect to the 
 "extended basis" generated by a given set of $n$ linearly independant $n$-D basis 1-vectors 
 $\mathbf{e_1, e_2, \cdots e_n}$ and this tells us how to add and subtract multivectors, but 
 not how to \emph{multiply and divide} them. For that we will need the \emph{"geometric product"} and its associated "subproducts". 

    \subsection{Conflicting terminologies}

 Some authors such as Pavsik use the term '\emph{$k$-vector}' for what we will call a $k$-blade
 and 'multivector' for our $k$-vector (ie. a single-graded multivector), adopting the term 
 polyvector for our multivector.

\vspace{\baselineskip}
 
 The term $k$-vector is sometimes used in the literature to refer to a $k$-dimensional 1-vector. 
 However it is more common to spell the number so that, for example, four-vector typically denotes
 a 1-vector in a 4D spacetime; three-vector denotes a 1-vector in $\mathbb{R}^3$ and so on, and we
 will adopt this convention here.   

    \subsection{Blades}

Given an orthonormal basis (ie. a Cartesian coordinate frame) consisting of $n$
orthonormal N-D 1-vectors $\mathbf{e_1,\cdots e_n}$ for $\mathbb{R}_{n}$,
we can consider multivectors as elements of $\mathbb{R}^{2^n}$, 
ie. as $2^n$-dimensional 1-vectors,

\vspace{\baselineskip}

 We use the notation $\mathbf{e}_{ij\cdots m}= \mathbf{e_i e_j\cdots e_m}$
 where $i,j,\cdots m$ are distinct indices. If we relax the distinctness 
 restriction in the case of $n$ indices we have 
 $\mathbf{e}_{ij\cdots m}= \mathbf{e_1 e_2\cdots e_n} \epsilon_{ij\cdots m}$, 
 where scalar $\epsilon_{ij\cdots m}$ is the traditional Levi-Civita
 "alternating tensor", returning $$\pm 1$ or 0 according as whether ordered set 
 $\{i,j,\cdots,m\}$ is an even/odd, or not a, permutation of ordered set 
 $\{1,2, \cdots, n \}$.

 \vspace{\baselineskip}
 
 If all $2^n$ coefficients of the basis blades arising from a given orthonormal basis are integers ( eg. $5 + 3\mathbf{e_1} - 7\mathbf{e_2\wedge e_3}$)
 then we will say the multivector is \emph{integer-coefficiented}
 with regard to that basis. If the coeffients are further 
 restricted to $\{ \pm1, 0\}$ then the resulting $32n$
 "discrete" multivectors are said to be \emph{Mantheyian}
 with regard to the basis. 
 

    \subsection{Specific Subcomponentes}

We will write $\mathbf{A}^{ij\cdots m}$ 
or $\mathbf{A}_{[ij\cdots m]}$ for the coefficient of $\mathbf{e}_{ij\cdots m}$
in multivector $\mathbf{A}$. We will seldom have to raise individual components to
integer powers and this raised suffix notation has definite advantages.

  \vspace{\baselineskip}
  
We will refer to the integer $(2j+2k+...+2m)/2$ as the \emph{bitwise enumerator}
for the extended basis blade $\mathbf{e}_{jk\cdots m}$. 
The ½ arises because we labelled our first frame vector $\mathbf{e_1}$ 
rather than $\mathbf{e}_0$. 
We will denote coordinates or blades indexed by a particular bitwise enumerator $n$
by $A_{[.n.]}$. Thus eg.

\begin{align*}
    &\mathbf{e}_{[.0.]}=1  \\
    &\mathbf{e}_{[.1.]}=\mathbf{e_1}  \\
    &\mathbf{e}_{[.6.]}=\mathbf{e_{23}} \\
\end{align*}

And so on. In general we have:

\begin{align*}
    \mathbf{A} = \sum_{i=0}^{2^n-1} a_{[.i.]}\mathbf{e}_{[.i.]}
\end{align*}

 \vspace{\baselineskip}
 
\textcolor{gray}{I've decided to adapt this a bit, because here he was essentially defining
the \emph{\textbf{grade selection operator}.}} We'll use the notation $\langle \mathbf{A} \rangle_k$
for the $k-$th component of a multivector, and the notation:

\begin{align*}
   \langle \mathbf{A} \rangle_{i,j,\cdots m} = \langle \mathbf{A} \rangle_i 
   +\langle \mathbf{A} \rangle_j + \cdots + \langle \mathbf{A} \rangle_m
\end{align*}

We say $\mathbf{A}$ is \emph{pure} or \emph{$k$-pure} if $\langle \mathbf{A} \rangle_k = \mathbf{A}$.
By an \emph{impure $k$-vector} we mean a multivector having a nonzero $k$-vector component
and possible nonzero $j-$vector components for $j<k$. \textcolor{gray}{In other words,
we may say a ´pure´ vector is simply a linear superposition of $k-$blades, or a 
vector of homogeneous grading}.

 \vspace{\baselineskip}
 
If we wish to specify a multivector as having only components of grades 2,3 and 5, for example, 
we will refer to a $<2;3;5>$-vector. A $<k>-$vector is thus a pure $k$-vector. 
A $<0;1>$-vector is sometimes known as a \textbf{\emph{paravector}}, particularly when $n=3$.
We will use the notations:

\begin{align*}
    &\langle \mathbf{A} \rangle_{\text{even}} =  \langle \mathbf{A} \rangle_{0,2,\cdots 2\cdots(n/2)} \\
    &\langle \mathbf{A} \rangle_{\text{odd}} =  \langle \mathbf{A} \rangle_{1,3,\cdots 2\cdots(n/2)+1}
\end{align*}

To denote the multivector obtained by taking only the even and odd grade components of 
$\mathbf{A}$, respectively. \textcolor{gray}{This is known as restricting to the ´odd´ or
´even´ subalgebras of a Clifford Algebra, which is important for defining spinors.}
    
    \subsection{Zeroes}

The degenerate (zero magnitude) $k$-blade is denoted by $0k$.
All degenerate blades are considered equivalent ( $0 = 0_1 = 0_2 = \cdots = 0_n = 0$ ), 
and this zero is the sole equivalence. 
Note that within a computational context (\emph{finite precision arithmetic}) we will 
typically be checking for proximity rather than equivalence with zero. It is often worth keeping 
track of the "grades" of expected zeros.

 \vspace{\baselineskip}
 
We are frequently interested in issues such as whether the square of a 
multivector is a pure scalar ($\mathbf{A}^2 = x\in \mathbb{R}$), 
but in practice this may mean checking that any residual nonzero coordinates are a "negligable" 
proportion of the scalar part, which can be problematic if the scalar part is also small.
We define the \emph{sparsity} of a multivector with respect to a given basis as the number of zero 
coefficients (coordinates) in its representation in that basis. 

    \subsection{Inverse Frames}

Suppose we have a possibly non-orthonormal linearly independant basis of $n$ $n$-D 1-vectors providing 
a coordinate frame (ie. a set of axies) $\mathbf{E=(e_1,e_2,...,e_N)}$ for 
$V^n$. We can construct a reciprocal or inverse frame $\mathbf{(e^1,e^2,...,e^N)}$ 
so that $\mathbf{e_i\cdot e^j} = 1$ when $i=j$ and 0 else where. 
($\cdot$) is the traditional scalar ("dot") product of two 1-vectors.

 \vspace{\baselineskip}
 
If $\mathbf{E}$ is orthogonal then provided $\mathbf{e_i}^2\ne0$, 
we can set $\mathbf{e_k} = \mathbf{e_k}^{-2}\mathbf{e_k}$. More generally, 
we require 
\begin{align*}
    \mathbf{e^k} \equiv (-1)^{k-1}(\mathbf{e_1\wedge \cdots \wedge e_{k-1} \cdots e_{k+1} \wedge
    \cdots e_n}), \quad i=\mathbf{e}_{12\cdots n}
\end{align*}

Though this may not make much sense to the reader till he is more familiar 
with $\wedge$ and the \emph{pseudoscalar} $i$ discussed later.
We define a notation $\mathbf{e}^{ij\cdots m} = \mathbf{e_i \wedge e_j \cdots e_m}$.

 \vspace{\baselineskip}
 
If $\mathbf{E}$ is orthogonal (ie. $\mathbf{e_i \cdot e^j}=0$ for $i\ne j$) 
then (using the \emph{geometric product} defined below) we have 
$\mathbf{e_k}\mathbf{e^k} = 1$ (provided $\mathbf{e_k}^2 \ne 0$), 
but in general $\mathbf{e_k e^k}$ has a non-zero 2-blade component because 
$\mathbf{e_k \wedge e^k} \ne 0$.

 \vspace{\baselineskip}
 
$\mathbf{E}$ induces both the \textbf{coordinate expression:}

\begin{align*}
    \mathbf{x} = \sum_{i=0}^n x^i \mathbf{e}_i, \quad x^i = \mathbf{e^i}\mathbf{x}
\end{align*}

And the \textbf{reciprocal coordinate expression:}

\begin{align*}
    \mathbf{x} = \sum_{i=0}^n x_i \mathbf{e}^i, \quad x_i = \mathbf{x}\mathbf{e_i}
\end{align*}

Where $\mathbf{e_i}$ is the geometric multiplier that 
"seperates" 1-vector $\mathbf{x}$ into:

\begin{align*}
    \mathbf{x}=x^i + \sum_{i \ne j} x^j \mathbf{e^i} \wedge \mathbf{e_j}
\end{align*}

\textcolor{gray}{In regards to a quick proof, notice the terms on the right
will always be of the form:}

\begin{align*}
   \color{gray} \mathbf{x} = x^0 &\color{gray}+x^1 \mathbf{e^0} \wedge \mathbf{e_1} \\
    &\color{gray}+ x^2 \mathbf{e^0} \wedge \mathbf{e_2} \\
    &\color{gray}+ \cdots \\
    \color{gray}x^1 &\color{gray}+ \textbf{}x^0 \mathbf{e^1} \wedge \mathbf{e_0} \\
    &\color{gray}+ x^2 \mathbf{e^1} \wedge \mathbf{e_2} \\
    &\color{gray}+ \cdots \\
    \color{gray}x^n &\color{gray}+ x^1 \mathbf{e^n} \wedge \mathbf{e_1} \cdots 
\end{align*}

\textcolor{gray}{Therefore cancelling each other out and only leaving the actual
vector component $x^i$.}

 \vspace{\baselineskip}

In $\mathbb{R}^n:$ 

\begin{itemize}
    \item An orthonormal frame is \emph{self-inverse} ($\mathbf{e^i=e_i}, \ x^i=x_i$)
    \item A general frame, 
expressed as an $n\times n$ matrix $\mathbf{E}$ with respect to a fixed orthonormal frame $\mathbf{F=(f_1,f_2, \cdots f_n)}$ in conventional manner via ( $\mathbf{e_i} =\Sigma_{i=1}^n E_{ji}f_j$ ) has as its reciprocal frame the frame having matrix $(\mathbf{E}^{-1})^T = (\mathbf{E}^T)^{-1}$ with respect to $\mathbf{F}$, ie. the inverse transpose matrix.
\end{itemize}

 \vspace{\baselineskip}
 
Letting Eij º ei¿ej  and Eij º ei¿ej ,  we have xj = åi=1N Eijxi     ;      xj = åi=1N Eijxi .
The N×N symmetric matrices {Eij} and {Eij} are related by {Eij} = {Eij}-1   where -1 is  the 
conventional matrix inverse. 
    
    \subsubsection{Inverse Frame Units}
    
We discuss the mathematically ticklish issue of inverse frames here because we are discussing coordinate representations.

 \vspace{\baselineskip}

If frame vectors are assigned \textbf{units}, $\mathbf{e}_1$ 
having length $5\si{m}$ say, then $\mathbf{e}^1$ 
must be regarded as having "length" $5^{-1} \si{m}^{-1}$
so that $\mathbf{e_i \cdot e^i} = 1 \si{m}^0$ is dimensionless. 
Coordinates $x^i =\mathbf{e}^i\lrcorner\mathbf{x}$ are then unitless while reciprocal coordinates 
$x_i =\mathbf{e}_i\lrcorner\mathbf{x}$ have units $\si{m}^2$.

    \subsubsection{Extended Inverse Frame}
    
Given an extended basis $\{ \mathbf{e}_{[.i.]} : 0 <2^n \}$, 
we can construct an extended pureblade inverse frame ${\mathbf{e}^{[.i.]}}$
which satisfies $\mathbf{e}_{[.i.]}\cdot \mathbf{e}^{[.j.]}=
\langle (\mathbf{e}_{[.i.]}\mathbf{e}^{[.j.]}) \rangle_0 = 1$ 
when $i=j$ and 0 else. 
    
    \section{The Geometric Product}

    \epigraph{He who can propery define and divide is to be considered a god.}{\textit{Plato}}

To make $\mathcal{R}_n$ a linear space we define a multiplication with the following rules:

\begin{tcolorbox}[colback=white, colframe = purple!60!black, title=\textbf{Geometric Product Rules} ]

\begin{itemize}
    \item \textbf{Linearity:} $\mathbf{a(b+c) = ab + ac}$
    \item \textbf{Associativity:} $\mathbf{a(bc) = (ab)c}$
    \item \textbf{Contraction:} $\mathbf{aa=} \text{Sig}(\mathbf{a}), \quad \text{Sig}(\mathbf{a})\in \mathbb{R} \label{geodef}$
\end{itemize}

\end{tcolorbox} 

Of principal interest here is the \textbf{contraction} $\text{Sig}(\mathbf{a})=\varepsilon|\mathbf{a}|^2$,
where $\varepsilon$ is the \textbf{signature} of $\mathbf{a}$, and can be $\pm 1$, 
and $|\mathbf{a}|$ is the common vector magnitude (´length') of 1-vector $\mathbf{a}$.
A vector is \emph{null} if $\mathbf{a}^2=0$. 

\textcolor{gray}{The ´contraction´ property is so defining as it comes from the construction, that is,
definition of, Clifford Algebras from said \emph{Tensor Algebras}, whereby choosing a quadratic
form to serve as the quotient of the ideal, in the way $\mathbf{T}(V)/\{I : x\otimes x = s\}$, forms
the geometric product rule. The same is applied to the exterior algebra previously mentioned, but
there $x\otimes x=0$.}

We

    \subsection{$\mathbb{R}^2$}

    % Please add the following required packages to your document preamble:
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|}
\hline
\cellcolor[HTML]{FAFF9F} &
  \cellcolor[HTML]{EDE2B7}1 &
  \cellcolor[HTML]{E1C4CF}$\mathbf{e_1}$ &
  \cellcolor[HTML]{D5A7E7}$\mathbf{e_2}$ &
  \cellcolor[HTML]{C88AFF}$\mathbf{e_{12}}$ \\ \hline
\cellcolor[HTML]{EDE2B7}1 &
  \cellcolor[HTML]{FAFF9F}1 &
  \cellcolor[HTML]{E1C4CF}$\mathbf{e_1}$ &
  \cellcolor[HTML]{D5A7E7}$\mathbf{e_2}$ &
  \cellcolor[HTML]{C88AFF}$\mathbf{e_{12}}$ \\ \hline
\cellcolor[HTML]{E1C4CF}$\mathbf{e_1}$ &
  \cellcolor[HTML]{E1C4CF}$\mathbf{e_1}$ &
  \cellcolor[HTML]{FAFF9F}1 &
  \cellcolor[HTML]{C88AFF}$\mathbf{e_{12}}$ &
  \cellcolor[HTML]{D5A7E7}$-\mathbf{e_2}$ \\ \hline
\cellcolor[HTML]{D5A7E7}$\mathbf{e_2}$ &
  \cellcolor[HTML]{D5A7E7}$\mathbf{e_2}$ &
  \cellcolor[HTML]{C88AFF}$-\mathbf{e_{12}}$ &
  \cellcolor[HTML]{FAFF9F}1 &
  \cellcolor[HTML]{E1C4CF}$\mathbf{e_1}$ \\ \hline
\cellcolor[HTML]{C88AFF}$\mathbf{e_{12}}$ &
  \cellcolor[HTML]{C88AFF}$\mathbf{e_{12}}$ &
  \cellcolor[HTML]{D5A7E7}$\mathbf{e_2}$ &
  \cellcolor[HTML]{E1C4CF}$-\mathbf{e_1}$ &
  \cellcolor[HTML]{FAFF9F}-1 \\ \hline
\end{tabular}
\caption{The Cayley table for the $\mathbb{R}^2$ geometric product. Notice the anticommutative orthogonal bivectors
$\mathbf{e_{12/21}}$ and how it squares to -1, as the imaginary unit.}
\label{tab:my-table}
\end{table}
    
    \subsection{$\mathbb{R}^{1,1}$}
    
    \subsection{$\mathbb{R}^3$}

We can tabulate the geometric product for $\mathcal{R}_3$
with respect to a basis derived from a standard orthonormal
basis $\{ \mathbf{e_1,e_2,e_3}\}$ for $\mathbb{R}^3$:


\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|l|l|l|l|}
\hline
\cellcolor[HTML]{FAFF9F}\textbf{} &
  \cellcolor[HTML]{F4F0AB}\textbf{1} &
  \cellcolor[HTML]{EDE2B7}\textbf{e1} &
  \cellcolor[HTML]{E7D3C3}\textbf{e2} &
  \cellcolor[HTML]{E1C4CF}\textbf{e3} &
  \cellcolor[HTML]{DBB6DB}\textbf{e23} &
  \cellcolor[HTML]{D5A7E7}\textbf{e31} &
  \cellcolor[HTML]{CE99F3}\textbf{e12} &
  \cellcolor[HTML]{998AFF}\textbf{e123} \\ \hline
\cellcolor[HTML]{F4F0AB}\textbf{1} &
  \cellcolor[HTML]{F4F0AB}\textbf{1} &
  \cellcolor[HTML]{EDE2B7}\textbf{e1} &
  \cellcolor[HTML]{E7D3C3}\textbf{e2} &
  \cellcolor[HTML]{E1C4CF}\textbf{e3} &
  \cellcolor[HTML]{DBB6DB}\textbf{e23} &
  \cellcolor[HTML]{D5A7E7}\textbf{e31} &
  \cellcolor[HTML]{CE99F3}\textbf{e12} &
  \cellcolor[HTML]{998AFF}\textbf{e123} \\ \hline
\cellcolor[HTML]{EDE2B7}\textbf{e1} &
  \cellcolor[HTML]{EDE2B7}\textbf{e1} &
  \cellcolor[HTML]{F4F0AB}\textbf{1} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{CE99F3}\textbf{-e12}} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{D5A7E7}\textbf{e31}} &
  \cellcolor[HTML]{998AFF}\textbf{e123} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{E1C4CF}\textbf{e3}} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{E7D3C3}\textbf{-e2}} &
  \cellcolor[HTML]{DBB6DB}\textbf{e23} \\ \hline
\cellcolor[HTML]{E7D3C3}\textbf{e2} &
  \cellcolor[HTML]{E7D3C3}\textbf{e2} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{CE99F3}\textbf{e12}} &
  \cellcolor[HTML]{F4F0AB}\textbf{1} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{DBB6DB}\textbf{-e23}} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{E1C4CF}\textbf{-e3}} &
  \cellcolor[HTML]{998AFF}\textbf{e123} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{EDE2B7}\textbf{e1}} &
  \cellcolor[HTML]{D5A7E7}\textbf{e31} \\ \hline
\cellcolor[HTML]{E1C4CF}\textbf{e3} &
  \cellcolor[HTML]{E1C4CF}\textbf{e3} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{D5A7E7}\textbf{-e31}} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{DBB6DB}\textbf{e23}} &
  \cellcolor[HTML]{F4F0AB}\textbf{1} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{E7D3C3}\textbf{e2}} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{EDE2B7}\textbf{-e1}} &
  \cellcolor[HTML]{998AFF}\textbf{e123} &
  \cellcolor[HTML]{CE99F3}\textbf{e12} \\ \hline
\multicolumn{1}{|l|}{\cellcolor[HTML]{DBB6DB}\textbf{e23}} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{DBB6DB}\textbf{e23}} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{998AFF}\textbf{e123}} &
  \cellcolor[HTML]{E1C4CF}\textbf{e3} &
  \cellcolor[HTML]{E7D3C3}\textbf{-e2} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{F4F0AB}\textbf{-1}} &
  \cellcolor[HTML]{CE99F3}\textbf{e12} &
  \cellcolor[HTML]{D5A7E7}\textbf{-e31} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{EDE2B7}\textbf{-e1}} \\ \hline
\multicolumn{1}{|l|}{\cellcolor[HTML]{D5A7E7}\textbf{e31}} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{D5A7E7}\textbf{e31}} &
  \cellcolor[HTML]{E1C4CF}\textbf{-e3} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{998AFF}\textbf{e123}} &
  \cellcolor[HTML]{EDE2B7}\textbf{e1} &
  \cellcolor[HTML]{CE99F3}\textbf{-e12} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{F4F0AB}\textbf{-1}} &
  \cellcolor[HTML]{DBB6DB}\textbf{e23} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{E7D3C3}\textbf{-e2}} \\ \hline
\multicolumn{1}{|l|}{\cellcolor[HTML]{CE99F3}\textbf{e12}} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{CE99F3}\textbf{e12}} &
  \cellcolor[HTML]{E7D3C3}\textbf{e2} &
  \cellcolor[HTML]{EDE2B7}\textbf{-e1} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{998AFF}\textbf{e123}} &
  \cellcolor[HTML]{D5A7E7}\textbf{e31} &
  \cellcolor[HTML]{DBB6DB}\textbf{-e23} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{F4F0AB}\textbf{-1}} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{E1C4CF}\textbf{-e3}} \\ \hline
\multicolumn{1}{|l|}{\cellcolor[HTML]{998AFF}\textbf{e123}} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{998AFF}\textbf{e123}} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{DBB6DB}\textbf{e23}} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{D5A7E7}\textbf{e31}} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{CE99F3}\textbf{e12}} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{EDE2B7}\textbf{-e1}} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{E7D3C3}\textbf{-e2}} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{E1C4CF}\textbf{-e3}} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{F4F0AB}\textbf{-1}} \\ \hline
\end{tabular}
\caption{The Cayley table for the $\mathbb{R}^3$ geometric product.}
\label{tab:my-table}
\end{table}

Similar tables can be constructed for higher $n$. \textcolor{gray}{The number of basis elements in
$\mathcal{Cl}(n)$ is given by a row in \emph{Pascal's Triangle} because it counts the simplex size of
such a dimension, then for $n=3$ we have 1 scalar, 3 basis vectors,
3 basis bivectors, a 1 trivector, yielding a Cayley table of size $8\times 8$. However, for $n=4$, we'd have
a Pascal sequence (1,4,6,4,1), therefore yielding a table of $16\times16$. Indeed, the $n$-th row of the triangle
contains $2^n$ components, and therefore for $\mathcal{Cl}(8)$ we'd have a table of $256\times 256$.}

\vspace{\baselineskip}

What is happening here is that $\mathbf{e}_{[.j.]}\mathbf{e}_{[.k.]} = \pm \mathbf{e}_{[.j\textbf{XOR} k.]}$ where the
sign is determined both by how many commutations we have to do to bring the common $\mathbf{e}_i$ together,
and the signatures of the common $\mathbf{e}_i$.

\vspace{\baselineskip}

Writing $\mathbf{e}_{123}=i$, we see from the above table tha $i$ commutes with all multivectors and
satisfies $i^2=-1$. We also observe that $\mathbf{a\wedge b} = i(\mathbf{a\times b}) = (\mathbf{a \times b})i$ where
$\mathbf{a \times b}i^{-1}$ is the conventional $\mathbb{R}^3$ \textbf{\emph{vector cross product}} and that
$i\mathbf{a}$ spans the plane normal to $\mathbf{a}$. We also have that:

\begin{align*}
    \mathbf{a\wedge b \wedge c} = (\mathbf{a} \cdot (\mathbf{b \times c}))i
\end{align*}

\textcolor{gray}{The cross product is one of the simplest Lie Products that abide by the group $SO(3)$,
the algebra of rotations in 3D space, and therefore only exists in 3 and 7 dimensional spaces.}

\vspace{\baselineskip}

We note in passing that the subspace $\mathcal{R}_{3+}$ consisting of all 3D multivectors having
no odd grade component, i.e the space of multivectors of the form $a+b\mathbf{e_{12}}+c\mathbf{e_{23}}+d\mathbf{e_{31}}$,
is closed under the geometric product, and is isomorphic to the quaternion space $\mathcal{Q}$, as is
$\mathcal{R}_{0,2}$ for $a+bi+ck+dj$.

    \subsubsection{Biquaternions $\mathbb{H}^2$}

We further note that a general $\mathcal{R}_3$ multivector can 
be uniquely expressed as $(a + \mathbf{A}_{<2>}) + (b + \mathbf{B}_{<2>})
\mathbf{e}_{123}$ where $a,b$ are scalars and $\mathbf{A}_{<2>},\mathbf{B}_{<2>}$
are pure $\mathcal{R}_3$ bivectors. 
An alternative biquaternion (aka. \emph{complex four-vector} aka. \emph{Pauli spinor})
representation of $\mathcal{R}_3$ 
sets $i=\mathbf{e}_{123}$ , $\sigma_1=\mathbf{e}_1$,
$\sigma_2=\mathbf{e}_2, \sigma_3=\mathbf{e}_3$
(satisfying $\sigma_i \sigma_j = \epsilon_{ijk} i \sigma_k). 

 \vspace{\baselineskip}
 
The biquaternion $\mathbf{q}$ is equivalent to the $\mathcal{R}_3$
multivector $\mathbf{A}$:

\begin{align*}
    \mathbf{q} &= (a_0+b_0 i) + (a_1 + b_1 i)\mathbf{e_1} +
    (a_2+b_2i)\mathbf{e_2} + (a_3 + b_3i)\mathbf{e_3} \\
    &=  a_0 + a_1\mathbf{e_1} + a_2 \mathbf{e_2} + 
    a_3\mathbf{e_3} + b_1 \mathbf{e_{23}} + 
    b_2 \mathbf{e_{31}} + b_3 \mathbf{e_{12}} + b_0 \mathbf{e_{123}} = \mathbf{A}
\end{align*}

The product of two 3D 1-vectors is usually taken in this context 
to be $\mathbf{ab} = \mathbf{a\cdot b} + i(\mathbf{a \times b})$ 
which is equivalent to the $\mathcal{R}_3$ geometric product since 
$i(\mathbf{a \times b})= \mathbf{a \wedge b}$. 

    \subsection{Overview}

Given an orthonormal 1-vector basis $\mathbf{e_1, e_2, \cdots e_n}$, we can regard multivectors as complex weighted sums of the $2n$ basis blades, 
and such a \emph{"coordinates form"} makes addition and multiplication 
particularly straightforward. As an example computation consider (where we assume $\mathbf{(e_1)^2=(e_3)^2=1}$:

\begin{align*}
    (1+2\mathbf{e_{13}})(3\mathbf{e_{1}}+4\mathbf{e_{1234}}) &= 
    3\mathbf{e_{1}}+4\mathbf{e_{1234}} + 6\mathbf{e_{13}}\mathbf{e_{1}}
    + 8\mathbf{e_{13}}\mathbf{e_{1234}}   \\
    &= 3\mathbf{e_{1}}+4\mathbf{e_{1234}} + 6\mathbf{e_{13}}\mathbf{e_{1}} 
    + 8\mathbf{e_{13}}\mathbf{e_{1234}}   \\
    &= 3\mathbf{e_{1}}+4\mathbf{e_{1234}} + 6\mathbf{e_{1}e_3e_1} + 8\mathbf{e_{1}e_3e_1e_2e_3e_4} \\ 
    &= 3\mathbf{e_{1}}+4\mathbf{e_{1234}} - 6\mathbf{(e_{1})^2e_3} - 8\mathbf{e_3(e_1)^2e_2e_3e_4}   \\
    &= 3\mathbf{e_{1}}+4\mathbf{e_{1234}} - 6\mathbf{e_{3}} - 8e3e2e3e4\mathbf{e_3e_2e_3e_4}  \\ 
    &= 3\mathbf{e_{1}}+4\mathbf{e_{1234}} - 6\mathbf{e_{3}} + 8\mathbf{(e_3)e_2e_2e_4} \\  
    &= 3\mathbf{e_{1}}+4\mathbf{e_{1234}} - 6\mathbf{e_{3}} + 8\mathbf{e_{24}}
\end{align*}
 
The "geometric product" can thus be viewed as a purely computational construct in 
which we can think of sliding basis 1-vectors across eachother, introducing a sign 
flip for every basis 1-vector crossed, until another instance of the same base 
1-vector is encountered, whereupon the two 1-vectors "condense" into their 
$\pm1$ scalar signature. The grade of the product of an orthonormal basis $k$-blade
with an l-blade from the same basis is thus $\le k+l$.

 \vspace{\baselineskip}
 
Though we constructed our extended basis and defined the geometric product using the
outer product $\wedge$, we could have assumed only the traditional scalar vector dot
product to define basis orthonormality condition
$\mathbf{e}_i \cdot \mathbf{e}_j = \varepsilon_i \delta_{ij}$ 
where $\delta_{ij} = 1$ when $i=j$ and 0 else. 
Then, defined the extended basis elemnets by incorporating 1 and unordered 
pairs, triples, $\cdots$, and $n$-tuples of the 
$\mathbf{e}_i$ into the basis written as $\mathbf{e}_{ij\cdots m}$; and 
finally defined the geometric product of two basis elements 
$\mathbf{e}_{ij\cdots m}\mathbf{e}_{no\cdots r}$ 
logically in this "index hoping with sign tracking" manner

    \section{Pseudoscalars}

The $n$-vectors from a given $V^n$ are all equivalent apart from magnitude 
('scale', 'volume') and sign ('handedness'). They are accordingly known as 
\textbf{\emph{pseudoscalars}}. 
Conversely a nonzero pseudoscalar "spans" (and can be thought of as 
representing) $V^n$.

\textcolor{gray}{The insight that the pseudoscalar defines the dimension
of the space is central to many applications of geometric operators. Indeed,
one defines a $n$ dimensional geometric algebra by selecting all
vectors $a$ where $a\wedge i = 0$, as per the \emph{Universal Geometric Algebra}.}

 \vspace{\baselineskip}
 
Let $i \equiv i_n$ be the unit pseudoscalar $\mathbf{e}_{12\cdots n}$ 
for $\mathcal{R}_n$. $i$ satisifes:

\begin{align*}
    i^2 = (-1)^{\frac{1}{2}(n-1)n}
\end{align*}

And commutes with all multivectors if $n$ is odd. 
For even $n$ we have:

\begin{align*}
    i\mathbf{A}_k = (-1)^k \mathbf{A}_k i
\end{align*}

So that $i$ (anti)commutes with (odd)even blades.
We say a multivector is \emph{central} if it commutes with all other multivectors.
For even $n$, only scalars are central but for odd $n$ any $<0;n>$-multivector
(scalar plus pseudoscalar) is central.

\textcolor{gray}{Example: in 2D, $\alpha i = i \alpha$ but 
$i\mathbf{e_1} = -\mathbf{e_1}i$, and in 3D $(a+i)i = i(a+i)$, as
$ai+i^2 = ai + i^2$, therefore this is a syndrome of the fact that, really,
it's scalars that are unaffected by the pseudoscalar, as any other graded
blade will receive a minus sign by interaction.}


 \vspace{\baselineskip}
 
An $(n-1)$-vector is sometimes refered to as a \emph{pseudovector} 
but we favour the term \emph{hyperblade} here.
Taking the geometric product of a multivector with $i$ maps $k$-blades to 
$(n-k)$-blades and vice versa. In particular it maps scalars to 
pseudoscalars (and vice versa) and vectors to pseudovectors 
(and vice versa).

\textcolor{gray}{The same is true for the \emph{Hodge Dual} $\star$
in the theory of differential forms, it ´complements´ missing volume;
generally, it is a map between the outer and interior products.}

 \vspace{\baselineskip}
 
Note that a $k$-blade acts as a pseudoscalar when acting upon multivectors 
wholly contained within the space it spans. The geometric product of a 
pseudoscalar $i$ with a blade $\mathbf{a}_k$ 
contained in the subspace spanned by $i$ 
spans the subspace of $i$ complimentary (orthogonal) to subspace
$\mathbf{a}_k$. In 
particular, the geometric product of any blade with itself is a scalar.

 \vspace{\baselineskip}
 
The \emph{signature }$\varepsilon_{\mathbf{b}_k}$ of a blade 
$\mathbf{b}_k$ is the sign of $\mathbf{b_k}^2$ 
(or zero if $\mathbf{b}_k^2 = 0$, 
in which case the blade is said to be null). Recall the definition
of the geometric product \ref{geodef}.

    \subsection{Duality}

We define the \textbf{dual} of a multivector $\mathbf{A}$ 
with respect to a pseudoscalar $i$ spanning a space containing $\mathbf{A}$ by:

\begin{align}
\star\mathbf{A} \equiv \mathbf{A}i^{-1} = \mathbf{A}\lrcorner i^{-1}
    \label{dualdef}
\end{align}

\textcolor{gray}{Rather than $\mathbf{A}^*$, I've used the same symbol for the
\emph{Hodge Dual} $(\star)$ in the theory of differential forms, making certain 
equations more similar.}

\vspace{\baselineskip}

Where $(\lrcorner)$ is the \emph{contractive inner product} defined below. 
Some authors favour $\mathbf{A}i$, but if $i$ is a unit pseudoscalar the difference is only one of sign.

\vspace{\baselineskip}

$\star \mathbf{A}$ spans the subspace of $i$ "perpendicular" to pureblade $\mathbf{A}$.
If $\mathbf{B}$ is an unmixed (ie. odd or even) multivector, it will either 
\emph{commute} or \emph{anticommute} with $i$. For odd $n$, 
the pseudoscalar commutes with everything and we have: 

\begin{align*}
    (\star \mathbf{A})\mathbf{B} = \mathbf{A (\star B)} = \mathbf{\star(AB)}
\end{align*}

For even $n$, $i$ (anti)commutes with (odd) even multivectors and we have:
\begin{align*}
    (\star \widehat{\mathbf{A}} )\mathbf{B} = \mathbf{A}(\star\widehat{\mathbf{B}}) = \star(\mathbf{A}\widehat{\mathbf{B}})
\end{align*}

Where $\widehat{\mathbf{A}}$ is the \emph{grade involution conjugation} defined below.

\vspace{\baselineskip}

In the presence of a standard basis for $\mathcal{R}_n$, computing $\star\mathbf{A}$ 
for the unit pseudoscalar $i=\mathbf{e}_{123\cdots n}$ is a computationally trivial "shuffling" 
of coordinates requiring no numeric computations. For the bitwise ordering we have:

\begin{align*}
    (\mathbf{A}i^{-1})_{[.i.]} = \pm \mathbf{A}_{[.(i \textbf{XOR}(2^n -1)).]}
\end{align*}

Where the actual sign depends upon $n$ and the bitwise parity of $i$.
The \textbf{inverse dual} or \emph{undual} is defined by:

\begin{align}
    (\star \mathbf{A})^{-1} \equiv \star \mathbf{A} i^2 = \mathbf{A}i \\
    \star(\star \mathbf{A})^{-1} = \mathbf{A}
\end{align}

    \subsection{Centrality}

Any multivector $\mathbf{A}$ defines an algebra
$\text{Cent}(\mathbf{A})$, known as the \emph{centralizer} of $\mathbf{A}$ 
consisting of all multivectors that commute with $\mathbf{A}$.

    \section{Bivectors}

For $N\le4$ the squares of bivectors commute with even multivectors while for $n=4$ 
the pseudoscalar part of $\mathbf{b}^2$ is negated on commutaion with odd multivectors.

\vspace{\baselineskip}

Left multiplication by a bivector ($\mathbf{A} \rightarrow \mathbf{bA}$) 
casts scalars into $\mathbf{b}$. If $\mathbf{b}$ is a 2-blade, it rotates 
directions within $\mathbf{b}$ (by $\mathbf{b}$), while casting directions perpendicular
to $\mathbf{b}$ into trivectors. For $n=3$, it casts bivectors in $\mathbf{b}$ 
to scalars while bivectors normal to $\mathbf{b}$ are rotated by $\mathbf{b}$ 
and the pseudoscalar is cast to 1-vector $\star \mathbf{b}$.

\vspace{\baselineskip}

Right multiplication by a bivector $(\mathbf{A} \rightarrow \mathbf{Ab})$ has similar effects. 
Indeed, a 2-blade commutes with all multivectors in its dual space, which is not the case 
for a 1-vector. Consequently, it is occasionally advantageous to represent 3D 1-vectors by their bivector duals.

\vspace{\baselineskip}

Let $\mathbf{A}_{<0;2>}$ be a multivector having only bivector and scalar components:

\begin{align*}
    &\mathbf{A\lrcorner b}= \mathbf{A\cdot b} \\
    &\mathbf{b \lrcorner A = b \cdot A} + b_0\mathbf{A}
\end{align*}

b2×a sends ^(a,b2) to 0 and rotates ¯(a,b2) in b2 by ½p, scaling it by |b2|.

\vspace{\baselineskip}

The operation $(\mathbf{A} \rightarrow \mathbf{(bA)\cdot A})$ 
for nonnull $\mathbf{b}$ is interesting, casting ¯(a,b) to 0 and ^(a,b) into

\begin{align*}
    | \bot(\mathbf{A,b})|^2\mathbf{b} = |\mathbf{A\wedge b}|^2(\mathbf{b})^{-2}\mathbf{b} = |\mathbf{A\wedge b}|^2\mathbf{b}
\end{align*}

   

    \section{Matrix Representations}

Multivectors can be represented with matrices (physicists have done so, largely unknowingly, for 
decades) with the geometric product corresponding to the traditonal matrix product. But matrices 
are seldom the best way to impliment multivectors computationally and tend to obscure the 
underlying geometries. Nonetheless, we will describe some matrix representations here: partly to 
convince the more skeptical reader that multivectors do actually 'exist', and also to provide an 
alternate model for those who have philosophical difficulties with the concept of adding 
'differently graded' blades to form a composite 'entity'. 

\vspace{\baselineskip}

Geometric (multivector) algebra becomes the
algebra of a particular "form" of matrix, requiring only standard matrix multiply and inversion 
techniques. Such an approach is computationally profligate, but can sometimes provide alternative 
insights as well as quick-and-dirty programming applications exploiting existing matrix suites. 

    \subsection{$\mathbb{R}^{p,q,r}$ in $\mathbb{R}^{2^n \times 2^n}$}

With regard to a particular extended basis, an $n$-D multivector $\mathbf{A}$ can be expressed 
as a $2n$ dimensional real 1-vector. But as a function mapping multivectors to multivectors
$\mathbf{A}(x) = \mathbf{A}x$ , ie. a transform of $2n$-D 1-vectors, 
$\mathbf{A}$ can also be represented as a $2n\times 2n$ matrix.

\vspace{\baselineskip}

Taking $\mathbf{A}=a^0+a^1\mathbf{e}_1+a^2\mathbf{e}_2+a^{12}\mathbf{e}_{12}$
in $\mathbb{R}^{p,q,r}$ with $p+q+r=2$ for example, we have:

\begin{align*}
    \mathbf{Z}&=(a^0+a^1\mathbf{e}_1+a^2\mathbf{e}_2+a^{12}\mathbf{e}_{12})
    (x^0+x^1\mathbf{e}_1+x^2\mathbf{e}_2+x^{12}\mathbf{e}_{12})\\
    &=(x^0a^0 + \mathbf{e}_{+1} x^1a^1 + \mathbf{e}_{+2}a^2x^2 -
    \mathbf{e}_{+1}\mathbf{e}_{+2}x^{12}a^{12})\\
    &+( a^1x^0 + a^0x^1 + +\mathbf{e}_{+2}a^{12}x^2- \mathbf{e}_{+2} a^2x^{12})\mathbf{e}_1 \\
    &+( a^2x^0 + a^0x^2 - \mathbf{e}_{+1} a^{12} x^1 + \mathbf{e}_{+1}a^1x^{12})\mathbf{e}_2 \\
    &+( a^0x^{12} + a^1x^2 - a^2x^1 + a^{12} ) \mathbf{e}_{12}
\end{align*}

Which can be expressed in matrix form:

    \begin{align*}
    \begin{pmatrix}
    a^0 & e_{+1}a^1  & e_{+2}a^2 & -e_{+1}e_{+2}a^{12} \\
    a^1 & a^0 & e_{+2}a^{12} & -a^2\\
    a^2 & -e_{+1}a^{12} & a^0 & e_{+1}a^1 \\
    a^{12}& -a^2 & a^1 & a^0
    \end{pmatrix}
    \begin{pmatrix}
    x^0\\
    x^1\\
    x^2\\
    x^{12}
    \end{pmatrix}
    \end{align*}

Note that the leftmost column of the matrix representation of a contains the 1-vector representation of a, and 
that the matrix is symmetric apart from occasional sign changes determined by both the position in the matrix
and the signatures of the basis 1-vectors.

Note also that the matrix trace (sum of leading diagonal elements) of the matrix representatrion of a is the 
frame-invariant scalar 2Na0 = 2Na<0> .

\vspace{\baselineskip}

More generally we have a = (a,ae1,ae2,...,ai) reagradablde as 2N column vectors.  
The geometric product ab corresponds to conventional matrix multiplication of the matrix representations of a and b. Multivector 1 has as its matrix representation the 2N×2N identity matrix.

\vspace{\baselineskip}
 
If the pseudoscalar is central (all commuting) then we can express each multivector as a 2N-1 complex 1-vector and obtain a C2N-1×2N-1 complex matrix, which requires only half as many reals as a Â2N×2N matrix.
Far more compact matrix representors for multivectors are typically available. The following are all "maximally compact" in that they require precisely 2N real scalar parameters to hold a general N-D multivector. 

    
    \subsection{$\mathbb{R}^{2}$ in $\mathbb{R}^{2 \times 2}$}

    \begin{align*}
    \mathbf{1}=
    \begin{pmatrix}
    1 &0 \\
    0 &1
    \end{pmatrix}
    \quad
    \mathbf{e_1}=
    \begin{pmatrix}
     1 &0 \\
     0 &-1
    \end{pmatrix}
    \quad
    \mathbf{e_2}=
    \begin{pmatrix}
     0 &1 \\
     1 &0
    \end{pmatrix}
    \quad
    \mathbf{e_{12}}=
    \begin{pmatrix}
     0 &1 \\
     -1 &0
    \end{pmatrix}
    \end{align*}
    
    \subsection{$\mathbb{R}^{1,1}$ in $\mathbb{R}^{2 \times 2}$}

    \begin{align*}
    \mathbf{1}=
    \begin{pmatrix}
     1 &0 \\
     0 &1
    \end{pmatrix}
    \quad
    \mathbf{e_1}=
    \begin{pmatrix}
     0 &1 \\
     1 &0
    \end{pmatrix}
    \quad
    \mathbf{e_2}=
    \begin{pmatrix}
     0 &-1 \\
     1 &0
    \end{pmatrix}
    \quad
    \mathbf{e_{12}}=
    \begin{pmatrix}
     1 &0 \\
    0 &-1
    \end{pmatrix}
    \end{align*}
    
    
    \subsection{$\mathbb{R}^{3}$ in $\mathbb{C}^{2 \times 2}$}

    \begin{align*}
    \mathbf{1}=
    \begin{pmatrix}
    1 &0 \\
    0 &1
    \end{pmatrix}
    \quad
    \mathbf{e_1}=
    \begin{pmatrix}
     0 &1 \\
     1 &0
    \end{pmatrix}
    \quad
    \mathbf{e_2}=
    \begin{pmatrix}
     0 &-i \\
     i &0
    \end{pmatrix}
    \quad
    \mathbf{e_{12}}=
    \begin{pmatrix}
     1 &0 \\
     0 &-1
    \end{pmatrix}
    \end{align*}

% Please add the following required packages to your document preamble:
% \usepackage{multirow}
% \usepackage{graphicx}
% \usepackage[table,xcdraw]{xcolor}
% Beamer presentation requires \usepackage{colortbl} instead of \usepackage[table,xcdraw]{xcolor}
\begin{table}[]
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|
>{\columncolor[HTML]{C0C0C0}}c c|cccc|}
\hline
\multicolumn{2}{|c|}{\cellcolor[HTML]{C0C0C0}} &
  \multicolumn{4}{c|}{\cellcolor[HTML]{C0C0C0}$\mathbf{a}$} \\ \cline{3-6} 
\multicolumn{2}{|c|}{\multirow{\cellcolor[HTML]{C0C0C0}$\mathbf{ab}$}} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}1} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}$\sigma_1$} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{9AFF99}$\sigma_2$} &
  \cellcolor[HTML]{CBCEFB}$\sigma_3$ \\ \hline
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}} &
  \cellcolor[HTML]{EFEFEF}1 &
  \multicolumn{1}{c|}{\cellcolor[HTML]{EFEFEF}1} &
  \multicolumn{1}{c|}{$\sigma_1$} &
  \multicolumn{1}{c|}{$\sigma_1$} &
  $\sigma_1$ \\ \cline{2-6} 
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}} &
  \cellcolor[HTML]{FFCCC9}$\sigma_1$ &
  \multicolumn{1}{c|}{$\sigma_1$} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{FFCCC9}1} &
  \multicolumn{1}{c|}{$-i\sigma_3$} &
  $+i\sigma_2$ \\ \cline{2-6} 
\multicolumn{1}{|c|}{\cellcolor[HTML]{C0C0C0}} &
  \cellcolor[HTML]{9AFF99}$\sigma_2$ &
  \multicolumn{1}{c|}{$\sigma_2$} &
  \multicolumn{1}{c|}{$+i\sigma_3$} &
  \multicolumn{1}{c|}{\cellcolor[HTML]{9AFF99}1} &
  $-i\sigma_1$ \\ \cline{2-6} 
\multicolumn{1}{|c|}{\multirow{\cellcolor[HTML]{C0C0C0} $\mathbf{b}$}} &
  \cellcolor[HTML]{CBCEFB}$\sigma_3$ &
  \multicolumn{1}{c|}{$\sigma_3$} &
  \multicolumn{1}{c|}{$-i\sigma_2$} &
  \multicolumn{1}{c|}{$+i\sigma_1$} &
  \cellcolor[HTML]{CBCEFB}1 \\ \hline
\end{tabular}%
}
\caption{The \emph{Pauli Algebra} of physical three-dimensional space. Notice how this Cayley table has a skew-symmetric structure corresponding to the SO(3) structure, which is inherited by objects such as the \emph{magnetic field}, which is itself composed of spatial bivectors.}
\label{tab:my-table}
\end{table}

\vspace{\baselineskip}

Note that $\text{adj}(\sigma_i) = -\sigma_i$, while $\text{adj}(1) = 1$,
where $\text{adj(A)}$ is the traditional $2\times 2$ matrix adjoint, corresponds
to a biquaternian \emph{"vector conjugation"}. The $\sigma_i$ and $1$ act as a basis for the 
full $\mathbb{C}^{2\times 2}$ algebra
of complex $2\times 2$ matrices since:

\begin{align*}
\begin{pmatrix}
 a &b \\
 c &b
\end{pmatrix}
=
\frac{1}{2}\left[ (a+d)\mathbf{1}+(b+c)\sigma_1+(b-c)\sigma_2+(a-d)\sigma_3  \right ]
\end{align*}

Each element of the \emph{\textbf{special unitary group}} $SU(2)$ of all
$\mathbb{C}^{2\times 2}$ \emph{unitary} ($\mathbf{AA^{\dagger}=A(A^{-T}}=1$) matrices
having unit positive determinant can be expressed via:

\begin{align*}
    U= \frac{1}{2} \sum_{i=1}^3 \alpha_i \mathbf{e}_i, \quad \alpha_i \in \mathbb{R}^+
\end{align*}

The vectors $\mathbf{e_1, e_2, e_3}$ are denominated \emph{\textbf{generators}}
of $SU(2)$, so we can geometrically view $SU(2)$ as the space of exponentiated
$\mathbb{R}^3$ 1-vectors. It is also isomorphic to the group $SO(3)$ of all
orthogonal ($\mathbf{AA^T}=1$) $\mathbb{R}^{3\times 3}$ matrices, with $U$ being:

\begin{align*}
    \begin{pmatrix}
 z &x-iy \\
 x+ai &z
\end{pmatrix}
=
\begin{pmatrix}
 z &\bar{x} \\
 x &z
\end{pmatrix}
\end{align*}

\vspace{\baselineskip}

Which is equivalent to $\mathbf{Ax}$ where $\mathbf{x} = {x,y,z}^T$, yielding
the quaternion representation of $\mathbb{R^3_+} \simeq SO(3)$, that is, an
equivalence of unit 4-dimensional vectors.

    
    \subsection{$\mathbb{R}^{3,1}$ in $\mathbb{R}^{4 \times 4}$}

\begin{align*}
    1 =
\begin{pmatrix}
  1&  0&  0&0 \\
  0&  1&  0&0 \\
  0&  0&  1&0 \\
  0&  0&  0&1
\end{pmatrix}
\quad
\mathbf{e}_1=
\begin{pmatrix}
  1&  0&  0&0 \\
  0&  -1&  0&0 \\
  0&  0&  -1&0 \\
  0&  0&  0&1
\end{pmatrix}
\quad
\mathbf{e}_2=
\begin{pmatrix}
  0&  1&  0&0 \\
  1&  0&  0&0 \\
  0&  0&  0&1 \\
  0&  0&  1&0
\end{pmatrix}
\\
\\
\quad
\mathbf{e}_3=
\begin{pmatrix}
  0&  0&  1&0 \\
  0&  0&  0&-1 \\
  1&  0&  0&0 \\
  0&  -1&  0&0
\end{pmatrix}
\quad
\mathbf{e}_4=
\begin{pmatrix}
  0&  0&  -1&0 \\
  0&  0&  0&1 \\
  1&  0&  0&0 \\
  0&  -1&  0&0
\end{pmatrix}
\end{align*}

These satisfy the $\mathbf{e}_i^2=-1$ and $\mathbf{e}_{ij}=-\mathbf{e}_{ji},
i\neq j$
requirements, as checked by \ref{Lounesto1}. Note that the 1-vector
representors are traceless and have determinant $\pm1$. Transpose negates
only $\mathbf{e}_4$ and corresponds to \emph{geometric Hermitian conjugation}.
    
    \subsection{$\mathbb{R}^{4,1}$ in $\mathbb{C}^{4 \times 4}$}

\begin{align*}
1 =
\begin{pmatrix}
  1&  0&  0&0 \\
  0&  1&  0&0 \\
  0&  0&  1&0 \\
  0&  0&  0&1
\end{pmatrix}
\quad
\mathbf{e}_1=
\begin{pmatrix}
  0&  0&  0&i \\
  0&  0&  i&0 \\
  0&  -i&  0&0 \\
  -i&  0&  0&0
\end{pmatrix}
\quad
\mathbf{e}_2=
\begin{pmatrix}
  0&  0&  0&-1 \\
  0&  0&  -1&0 \\
  0&  -1&  0&0 \\
  1&  0&  0&0
\end{pmatrix} \\
\\
\quad
\mathbf{e}_3=
\begin{pmatrix}
  0&  0&  i&0 \\
  0&  0&  0&-i \\
  -i&  0&  0&0 \\
  0&  i&  0&0
\end{pmatrix}
\quad
\mathbf{e}_4=
\begin{pmatrix}
  0&  0&  1&0 \\
  0&  0&  0&1 \\
  1&  0&  0&0 \\
  0&  1&  0&0
\end{pmatrix}
\quad
\mathbf{e}_5=
\begin{pmatrix}
  -i&  0&  0&0 \\
  0&  -i&  0&0 \\
  0&  0&  i&0 \\
  0&  0&  0&i
\end{pmatrix}
\end{align*}

These satisfy the $\mathbf{e}_i^2=-1$ and $\mathbf{e}_{ij}=-\mathbf{e}_{ji},
i\neq j$
requirements. Once again, the 1-vector representants are traceless and have
a determinant of $\pm1$. Note that $\mathbf{e}_{12345}=i$ and 
$\mathbf{e}_{1234}=-i\mathbf{e}_5$, the pseudoscalar properties.

\vspace{\baselineskip}


    
    \subsection{$\mathbb{R}^{7}\simeq\mathbb{R}^{5,2}\simeq\mathbb{C}^{6} $ $\mathbb{C}^{8 \times 8}$}

We can form $\mathbb{C}_{8\times8}, \mathbf{e_1,e_2,\cdots,e_5}$ by taking their 
$\mathbb{C}_{4\times4}$ representaions and replacing every 1 with a $\mathbb{C}_{2\times2}$ 
matrix $\sigma_1$, every $i$ with $i\sigma_i$, and every 0 with 0. 
Now form the $\mathbb{C}_{8\times8}$ matrix $\mathbf{e}_6$ as the symmetric imaginary 
matrix formed by replacing each 1 in $\mathbb{C}_{4\times8}$ 1 with $\sigma_2$ and each 0 with 0. 

\vspace{\baselineskip}

This anticommutes with the $\mathbf{e_1,e_2,\cdots,e_5}$ as required and squares to 1. 
Finally we can form $\mathbf{e}_7$ of negative signature by replacing the 1 in 1 with $$-i \sigma_3$.
This anticommutes with the $\mathbf{e_1,e_2,\cdots,e_6}$ and squares to -1.

Note that $+\square$ unit 2-blade $\mathbf{e}_{67}$ is represented by 1 with the 1's replaced by 
$-i\sigma_2 \sigma_3= \sigma_1$. $\mathbf{e}_{12345}$$ by 1 with 1's replaced by 
$i\sigma_1$, and $\mathbf{e}_1234567$ by $i1$.

We can keep repeating this trick to create $\mathbb{C}_{16\times16}$ representations of an orthogonal 1-vector 
basis for $\mathbb{R}^{8,1}\simeq\mathbb{R}^{6,3}\simeq \cdots \mathbb{R}^{0,7}$,
$\mathbb{C}_{32\times32}$ representations for $\mathbb{R}^{11}\simeq\mathbb{R}^{9,2}\simeq \cdots \mathbb{R}^{1,10}$, $\mathbb{C}_{64\times64}$ representations for $\mathbb{R}^{12,1}\simeq \cdots \mathbb{R}^{0,13}$ and so on. 
    
    \subsection{Other matrix representations}

It can be shown that (where $\mathbb{Q}$ represents the space of quaternion matrices):

\begin{itemize}
    \item $\mathbb{R}^{0,4}\simeq \mathbb{R}^{4,0} \simeq \mathbb{Q}^{2\times 2}$
    \item $\mathbb{R}^{0,6}\simeq \mathbb{R}^{8\times 8}$
    \item $\mathbb{R}^{6,0}\simeq \mathbb{Q}^{4\times 4}$
    \item $\mathbb{R}^{7,0}\simeq \mathbb{R}^{5,2} \simeq \mathbb{C}^{8\times 8}$   
    \item $\mathbb{R}^{0,8}\simeq \mathbb{R}^{8,0} \simeq \mathbb{R}^{16\times 16}$
    
\end{itemize}

 But we do not provide example basies here. More generally, for even $p+q$, $\mathbb{R}_{p,q}$ 
 is isomorphic to either a real, complex, or quaternion matrix algbera. For odd $p+q$ 
 we sometimes have a sum (ordered pair) of two such matrix algebras. 

    \subsection{Adding blades}

Expressing multivectors in coordinate or matrix forms induces a natural addition of multivectors and provides a 
potent computational technique, but regarding multivector $\mathbf{A}$ as being the real weighted sum of the $2n$
outter product progency of $n$ elemental "generators"  $\mathbf{e}_1\mathbf{e}_2, \cdots \mathbf{e}_n$ 
or a particular real matrix risks loosing sight of duality. 

All the "information" in a blade $\mathbf{B}$ is also "encoded" in its dual $\mathbf{B}i^{-1}$ 
and we might equally naturally 
regard $\mathbf{A}$ as $\beta \mathbf{B} + \gamma\mathbf{C} + \cdots$ where 
$\beta = \beta_1 + \beta_2i + \cdots$ as a sum of "complex"-weighted blades, particularly for odd $n$ when
$i$ is central and (when $i^2=-1$) we have $\mathbb{R}^{p,q}\simeq\mathbb{C}^{p-1,1}
\simeq \mathbb{C}^{p,q-1}$. Because we can express 
$\mathbf{A}$ in terms of an extended basis we know that any multivector $\mathbf{A}$ 
can be expressed as a complex-weighted sum of $2^{n-1}$ blades, but, it is the 
\emph{minimal} number of blades necessasry that best "classifies" the multivector. 

\vspace{\baselineskip}

We define the positive integer spread of a multivector $\mathbf{A}$ to be the number of blades in its
"irreducible" real weighted summation.   The spreads of 1 and of $\mathbf{e}_{13} + \mathbf{e}_{23}
= (\mathbf{e}_1+\mathbf{e}_2) \wedge \mathbf{e}_3 $  are 1, for example. However the spread 
does not fully categorise the "complexity" of a because we don't know how many blades appear 
with their duals. We will refer to the Expression of a multivector $\mathbf{A}$ such 
as $\beta\mathbf{B}+\gamma \mathbf{C}$  of reduced spread 2 in coordinate form "smears" b and c together. The spread and reduced spreads are not apparent.

\vspace{\baselineskip}

Though the addition of multivectors is well defined and frame-independant, it is in many cases of somewhat dubious merit, particularly when adding high grade blades. Though the coordinate approach facilitates and encourages multivector additions and we will do so freely throughout this work, the strict mathematical purist is nonetheless correct to maintain an element of unease. Informally, multivectors are made for multiplying, not adding. 

    \chapter{Multivector Products}
    \section{Restricted products}

Given the geometric product we can define a large variety of \emph{partial-geometric}
or \emph{restricted products} by evaluating $\mathbf{ab}$ and then throwing away 
(zeroing the "coordinate" of) one or more 
particular blades. We might zero all the odd blades, for example, but more useful restricted 
products arise when the blades we zero are determined by the blades we multiply. The outter product 
is an example, with $\mathbf{a_k \wedge b_l} = \langle \mathbf{a_k b_l} \rangle_{<k+l>}$. 
Though all these products can in principle be implimented
using a geometric product primative, this is frequently grossly computationally inefficient and we 
will later see that one should generalise our encoded multivector product to "streamline" the 
evaluation of the more useful restricted products.

\vspace{\baselineskip}
 
There are two "outer" products and a plethora of potential "inner" products, most of which have 
something to commend them. All are restrictions of the geometric product in that their result for 
two extended basis blades is either the same as that of the geometric product or zero. From a
programmer's perspective, the contractive inner product $\lrcorner$ (defined below) is the most 
fundamental 
inner product. The reader should accordingly familiarise himself with $\lrconer$,
and also the Hestenes ($\cdot$)  product since it is ubiquitous in much of the literature,
often required in this work, and (unlike $\lrcorner$) is "dual" to the outer product.
The other inner produts ($\bullet, \llcorner,\lrcorner_{+}, \cdots$) are not used here and seldom
seen elsewhere. They may have their place in a particular application, however, and are defined here 
for completeness. 
    
    \section{The outer product}

We can define the \emph{outer, exterior} or \emph{wedge} product by:

\begin{align}
\mathbf{a_k \wedge b_l} = \langle \mathbf{a_k b_l} \rangle_{<k+l>}
    \label{wedgedef}
\end{align}

Where $\mathbf{a_k/a_l}$ are blades of any grade $k/l$, and thence extending to
multivectors by insisting on associativity and multilinearity.
Some identities are:

\begin{tcolorbox}[colback=white, colframe=blue!10!black, title=\textbf{Outer product identities} ]

\begin{itemize}

    \item $\mathbf{a}\wedge \gamma = \gamma \wedge \mathbf{a} = \gamma \mathbf{a}, \quad \gamma \in \mathbb{R}$
    
    \item $\mathbf{a_k \wedge a_l} = (-1)^{kl}\mathbf{b_l \wedge a_k}$
    
    \item $\mathbf{a\wedge b} = \frac{1}{2}(\mathbf{ab-ba}) \equiv \mathbf{a\times b}$
    \textcolor{gray}{(Commutator, not vector, product)}
    
    \item $\mathbf{a\wedge b_k} = \frac{1}{2}(\mathbf{ab_k} + (-1)^k\mathbf{b_k a} )$
    
    \item $\mathbf{b_k \wedge a} = (-1)^k \mathbf{a\wedge b_k}$
    
    \item $\mathbf{a_1 \wedge a_2 \cdots \wedge a_k} = k!^{-1} \sum_{ij\cdots m}\epsilon_{ij\cdots m} \mathbf{a_i a_j \cdots a_m}$, summing over all $k!$ permutations of $\{1,2\cdots n\}$
    
    \item $(\mathbf{a\wedge b})^2 = (\mathbf{a\lrcorner b})^2 - (\mathbf{a}^2)(\mathbf{b})^2 = -(\mathbf{a}^2\mathbf{b}^2) \sin(\theta)$, where $\theta$ is the angle between $\mathbf{a}$ and $\mathbf{b}$
    
    \item $\mathbf{(a_1 \wedge a_2 \cdots a_k)}^2 = (-1)^{ k(k-1)/2 } (k! \text{Vol. of simplex}(\mathbf{a_1, a_2, \cdots a_k}))^2  $
    
    \item $\mathbf{a_k \wedge a_k} = 0$ is true for a \emph{single} proper blade $\mathbf{a_k}$, but not for general $k-$vectors when $k>1$. Note for example, that a scalar (0-blade) has $a\wedge a= a^2$
\end{itemize}

\end{tcolorbox}


\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|c|c|
>{\columncolor[HTML]{C0C0C0}}c |
>{\columncolor[HTML]{C0C0C0}}c |
>{\columncolor[HTML]{C0C0C0}}c |
>{\columncolor[HTML]{C0C0C0}}c |}
\hline
\cellcolor[HTML]{FAFF9F}\textbf{} &
  \cellcolor[HTML]{F4F0AB}\textbf{1} &
  \cellcolor[HTML]{EDE2B7}\textbf{e1} &
  \cellcolor[HTML]{E7D3C3}\textbf{e2} &
  \cellcolor[HTML]{E1C4CF}\textbf{e3} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{DBB6DB}\textbf{e23}} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{D5A7E7}\textbf{e31}} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{CE99F3}\textbf{e12}} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{998AFF}\textbf{e123}} \\ \hline
\cellcolor[HTML]{F4F0AB}\textbf{1} &
  \cellcolor[HTML]{F4F0AB}\textbf{1} &
  \cellcolor[HTML]{EDE2B7}\textbf{e1} &
  \cellcolor[HTML]{E7D3C3}\textbf{e2} &
  \cellcolor[HTML]{E1C4CF}\textbf{e3} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{DBB6DB}\textbf{e23}} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{D5A7E7}\textbf{e31}} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{CE99F3}\textbf{e12}} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{998AFF}\textbf{e123}} \\ \hline
\cellcolor[HTML]{EDE2B7}\textbf{e1} &
  \cellcolor[HTML]{EDE2B7}\textbf{e1} &
  \cellcolor[HTML]{C0C0C0}\textbf{0} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{CE99F3}\textbf{e12}} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{D5A7E7}\textbf{e31}} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{998AFF}\textbf{e123}} &
  \textbf{0} &
  \textbf{0} &
  \textbf{0} \\ \hline
\cellcolor[HTML]{E7D3C3}\textbf{e2} &
  \cellcolor[HTML]{E7D3C3}\textbf{e2} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{CE99F3}\textbf{-e12}} &
  \cellcolor[HTML]{C0C0C0}\textbf{0} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{DBB6DB}\textbf{-e23}} &
  \textbf{0} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{998AFF}\textbf{e123}} &
  \textbf{0} &
  \textbf{0} \\ \hline
\cellcolor[HTML]{E1C4CF}\textbf{e3} &
  \cellcolor[HTML]{E1C4CF}\textbf{e3} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{D5A7E7}\textbf{-e31}} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{DBB6DB}\textbf{e23}} &
  \cellcolor[HTML]{C0C0C0}\textbf{0} &
  \textbf{0} &
  \textbf{0} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{998AFF}\textbf{e123}} &
  \textbf{0} \\ \hline
\multicolumn{1}{|l|}{\cellcolor[HTML]{DBB6DB}\textbf{e23}} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{DBB6DB}\textbf{e23}} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{998AFF}\textbf{e123}} &
  \cellcolor[HTML]{C0C0C0}\textbf{0} &
  \cellcolor[HTML]{C0C0C0}\textbf{0} &
  \textbf{0} &
  \textbf{0} &
  \textbf{0} &
  \textbf{0} \\ \hline
\multicolumn{1}{|l|}{\cellcolor[HTML]{D5A7E7}\textbf{e31}} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{D5A7E7}\textbf{e31}} &
  \cellcolor[HTML]{C0C0C0}\textbf{0} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{998AFF}\textbf{e123}} &
  \cellcolor[HTML]{C0C0C0}\textbf{0} &
  \textbf{0} &
  \textbf{0} &
  \textbf{0} &
  \textbf{0} \\ \hline
\multicolumn{1}{|l|}{\cellcolor[HTML]{CE99F3}\textbf{e12}} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{CE99F3}\textbf{e12}} &
  \cellcolor[HTML]{C0C0C0}\textbf{0} &
  \cellcolor[HTML]{C0C0C0}\textbf{0} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{998AFF}\textbf{e123}} &
  \textbf{0} &
  \textbf{0} &
  \textbf{0} &
  \textbf{0} \\ \hline
\multicolumn{1}{|l|}{\cellcolor[HTML]{998AFF}\textbf{e123}} &
  \multicolumn{1}{l|}{\cellcolor[HTML]{998AFF}\textbf{e123}} &
  \cellcolor[HTML]{C0C0C0}\textbf{0} &
  \cellcolor[HTML]{C0C0C0}\textbf{0} &
  \cellcolor[HTML]{C0C0C0}\textbf{0} &
  \textbf{0} &
  \textbf{0} &
  \textbf{0} &
  \textbf{0} \\ \hline
\end{tabular}
\caption{The Cayley table for the $\mathbb{R}^3$ for the outer product. }
\label{tab:my-table}
\end{table}

    
    \section{The "thin" outer product}

The ´thin´ outer product is define by $\wedge_t$:

\begin{align*}
    \mathbf{a\wedge_{t} b = a\wedge b} -
    \langle \mathbf{a} \rangle_0 \mathbf{b} -
    \langle \mathbf{b} \rangle_0 \mathbf{a} 
\end{align*}

It has the same multiplication table as the regular wedge,
but with the scalar row and column zeroed. It has no use in this book.
    
    \section{The contractive inner product}

We define the \textbf{emph{contractive inner product}}
(aka the \emph{Lounesto inner product} or \emph{left contraction}) ($\lrcorner$)
by:

\begin{align}
\mathbf{a_k\lrcorner b_l} = \langle \mathbf{a_kb_l} \rangle_{k-l}, \quad
\text{for } l\ge k, \ 0 \text{ else}
\end{align}

Where $\mathbf{a_k}$ and $\mathbf{b}_l$ are blades of any grade, and thence
extending to all multivectors by insisting on bilinearity, as all inner
products.

\vspace{\baselineskip}

Differently from the symmetric/regular inner product, it is neither
associative nor commutative due to the grade subtraction; that is:

\begin{align*}
    &\mathbf{a}\lrcorner\mathbf{a} = \mathbf{a}^2 \\
    &(\mathbf{a}\lrcorner\mathbf{a})\lrcorner\mathbf{b}=\mathbf{a}^2\mathbf{b} \\
    &\mathbf{a}(\mathbf{a\lrcorner b}) = 0
\end{align*}

For $k$-vectors, contraction with a 1-vector $\mathbf{a}$ corresponds to
orthogonal projection into the subspace $\star \mathbf{a}$, therefore,
$\mathbf{a\lrcorner b}$ is \textbf{the component factor of blade $\mathbf{b}$
perpendicular to 1-vector $\mathbf{a}$.} We have the identities:

\begin{tcolorbox}[colback=white, colframe=blue!10!black, title=\textbf{Contractive inner product identities} ]

\begin{itemize}

    \item a$\mathbf{\lrcorner b} = \mathbf{b}\lrcorner = a\mathbf{b}, \quad a\in \mathbb{R} $
    
    \item $\mathbf{a \lrcorner b} = \frac{1}{2}(\mathbf{ab+ba})$
    
    \item $\mathbf{ab} = \mathbf{a\lrcorner b} + \mathbf{a\wedge b} $
    
    \item $\mathbf{a\lrcorner b_k} = \frac{1}{2}(\mathbf{ab_k}-(-1)^k\mathbf{b_ka}, \quad k\ge0 $
    
    \item $\mathbf{a \lrcorner (b \wedge c)} = (\mathbf{a\lrcorner b})
    \wedge \mathbf{c} + \star\mathbf{b}\wedge (\mathbf{a \lrcorner c})$
    \textcolor{gray}{Classic ABC-BAC vector identity}
    
    \item $ \mathbf{a\lrcorner(b\lrcorner c) = (a\wedge b)\lrcorner c } $
    \textcolor{gray}{In-n-out swap rule; $\mathbf{a\lrcorner(a\lrcorner b)}=0$}
    
    \item $ \mathbf{a\lrcorner(b\lrcorner} i) = \mathbf{(a\wedge b)\lrcorner} i $
    
    \item $ \mathbf{a} \lrcorner (\mathbf{b_1 \wedge b_2 \cdots b_k})
    = \sum_{i=1}^k (-1)^{i+1} (\mathbf{a\lrcorner b_i}) (\mathbf{b_1\wedge \cdots \wedge b_{i-1}
    \wedge b_{i+1} \wedge \cdots \wedge b_{k} $ \textcolor{gray}{\textbf{Expanded
    $k$-blade contraction rule}}

    \item a

    \item $\mathbf{(a\wedge b)(a+b)} = (\mathbf{a^2 + a\lrcorner b)(a-b)}$
    
    \item $\mathbf{a} \lrcorner (\mathbf{b_k \wedge c} = 
    (\mathbf{a \lrcorner b_k}) \wedge \mathbf{c} + (-1)^k \mathbf{b_k}\wedge
    (\mathbf{a}\lrcorner \mathbf{c})$ 
    
\end{itemize}

\end{tcolorbox}



    \section{The Hestenes inner product}

ome authors favour the \emph{semi-symmetric} or \emph{semi-commutative inner product} 
(aka. \textbf{\emph{Hestenes inner product}}) ($\cdot$) defined by:

\begin{align}
    \mathbf{a_k \cdot b_l = \langle a_k b_l \rangle _{|l-k|}
\end{align}

Which in turn implies the symmetric product with a scalar:

\begin{align*}
    a\mathbf{ \cdot b_k \equiv b_k \cdot} a \equiv 0, \quad
    a \in \mathbb{R}
\end{align*}

Where $\mathbf{a_k/b_l}$ are any blades, and thence,
insisting on bilinearity it extends to all multivectors.
The result is neither associative or commutative, it is only
\emph{semi-symmetric} in that:

\begin{align*}
    \mathbf{a}_j\mathbf{b}_k =
    (-1)^{j(k-j)}\mathbf{b}_k \mathbf{a}_j, \quad j\le k
\end{align*}

Note that scalars (0-blades) $a,b$ satisfy $a\wedge b = 
a \lrcorner b = a\cdot b = ab$, therefore we may think
of scalars as ´self-orthogonal'. We have the identities:

\begin{tcolorbox}[colback=white, colframe=blue!10!black, title=\textbf{Hestenes inner product identities} ]

\begin{itemize}

    \item a$\mathbf{\lrcorner b} = \mathbf{b}\lrcorner = a\mathbf{b}, \quad a\in \mathbb{R} $
  
    
\end{itemize}

\end{tcolorbox}
    
    \section{The "fatdot" inner product}

A variation of the Hestenes inner product defined by:

\begin{align}
    \mathbf{a_k \circ b_l} = \langle \mathbf{a_k b_l} \rangle_{|k-l|}, \quad k,l\ge 0
\end{align}

This is sometimes known as the \emph{modified Hestenes} or simply
\emph{dot product}, but we will call it the \emph{fatdot product}
here to avoid confusion with the traditional (Hestenes) inner product .

\vspace{\baselineskip}

The multiplication table for · is the same as that for . except with regard to scalars. The "scalar" row and column are filled as for Ù , ie. according to 1·a  = a·1 = a    .
We can think of · as "abrogating" the "scalar handling" from Ù, reducing it to the "thin" outter product ^. Thus · and ^ provide an alternate "decomposition" of the geometric product to . and Ù which may be fruitful in some contexts but will not be exploited here.

    \section{The forced Euclidean contractive inner product}

The forced Euclidean contractive inner product ¿+ "overrides" the signatures of the vectors on which it operates. In a Euclidean space ÂN , ¿+ = ¿ .   In Âp,q , ¿+ is defined with regard to an orthonormal basis {ei} . We take 1¿+a º a and  ei¿+ej º |ei¿ej| and extend ¿+ bilinearly.
The tabulation of ¿+ with regard to the extended (multivector) basis for Âp,q is thus the tabulation for ¿ for Âp+q.

Although frame dependant, ¿+ is a useful product computationally, since one can sometimes simply "lift" a problem (such as computing meets and joins) in a nonEuclidean space into Euclidean space and solve it there.
We define the frame-dependant forced Euclidean geometric product by extending a¨+b º a¿+b + aÙb over Âp,q linearly and asscoiatively so that the tabulation of ¨+ for a given basis for Âp,q is the tabulation for ¨ for Âp+q.

We will later consider higher dimensional embeddings of UN obtained by adding "extension" dimensions e+ and e- to a basis and in such cases will often wish to retain the negative signature of the extender while forcing UN Euclidean. We represent the unextended forced Euclidean contractive product by ¿(+) and the unextended forced Euclidean geometeric product by ¨(+) . 
    
    \section{Commutator product}

The antisymmetric commutator or product is defined by a×b º ½(ab-ba). It is our first non-partial-geometric product, potentially containing blades not present in ab. [  aka. , without the ½ factor, as the Lie product ]
    × is nonassociative, and often represents the appropriate generalisation of the Â3 vector product ×.
    We have 

    \section{AntiCommutator product}
    \section{Scalar product}
    \section{Scalar-Pseudoscalar product}
    \section{Inversive product}
    \section{Delta products}

The greater delta product D (aka. disjoint) is a nonbilinear restrictive product returning the highest grade nonvanishing component of the geometric product, or zero if the geometric product is zero. This can be computed, albeit somewhat inefficiently, by calculating the geometric product keeping track of, or subsequently seeking, the highest grade attained by the blades in the product, and then restricting the product to only this grade. It is most commonly applied to blades and spans the "difference" between them, so for example ((e1+e2)Ùe34) D e235 = -e1245.   It is profoundly useful in constructing meets and joins.

Of course, calculating a delta product can present us with dilemas when small coordinate values are encountered. If a product ab has a large scalar part and some comparatively tiny 2-blade coordinates, is the highest "nonvanishing" grade zero or two? Are the small 2-vector coordinates a genuine geometrical artifact, or just "failed zeroes" arising from finite precision computations? What magnitudes may we discard? Our choice will substantively effect the grade and magnitude of aDb and our computations risk becoming more art than science. Nonetheless, the delta product is profoundly useful since in many cases in can be computed unambiguously and is then fundamental in constructing meets,  joints, and disjoints as described later.

We also have the lesser delta product returning the nonzero component of minimal grade adb = (ab)Min . 
    
    \section{Conjugative Products}

For every multivector conjugation ^ there are two associated restricted geometric products. One restricting to the "^-real" blades (b^ = b) , and one to the "^-imaginary" blades with b^=-b. 
    
    \section{Rescaled Product}

The recaled product is defined by a§b º (a_rsccb_rscc)_rscc where x_rscc is an arbitary, potentially frame dependant, computationally convenient rescaling normalisation of x such as division by the maximal absolute coordinate, or the sum of absolute coordinates, of x.in a particvular frame.. a§b is equivalent to a¨b apart from scale.

\vspace{\baselineskip}
    
The recalsed product is particularly useful when evaluating sandwich rotor products such as (½qa)↑ x (-½qa)↑ for large q when intermediary coordinates can aquire collosal magnitudes and rounding errors become significant. The evaluation of (½qa)↑ §  x § (-½qa)↑.is usually far better behaved ; and the correct scale can easily be recovered for non null x from , ((½qa)↑ x (-½qa)↑)2 =  x2 whenver x2 commutes with a .

    
    \section{Pure Product Rule}

\begin{align}
    \mathbf{a_k b_l} &= \langle \mathbf{a_k b_l} \rangle_{|k-l|}
    + \langle \mathbf{a_k b_l} \rangle_{|k-l|+2} + \cdots 
    + \langle \mathbf{a_k b_l} \rangle_{k+l} \\
    &= \sum_{m=0}^{ \frac{1}{2}(1+k-|k-l|)} \langle \mathbf{a_k b_l} \rangle_{|k-l|+2m}
\end{align}

Hence, the geometric product of two pure multivectors is either odd or even 
(as defined under Involution below) according as integer $k-l$ is odd or even. The
proof is that it is true for $k=1$, and then by induction on $k$ for single $k$-blade 
$\mathbf{a}_k$. Result follows by distributivity.
    
    \section{The Intersective Product}

he intersective product is a not a restricted geometric product and is frame dependant. It picks out the "shared axies" eg. e3Çe123=e3. In programming terms, while the geometeric product is based on the exclusive or (XOR) of the bitwise ennumeration indices, the intersective product is based on their logical AND. We bring in the sign changes due to commutations ( e2Çe12 = -e2) but not the signatures.

    We will later encounter the frame-independant meet operation aÇb and while we do have e[.j.]Ç[.k.]) = e[.j.]Ç[.k.]) = ±e[.j AND k.] we will see that Ç is not distributive across + for same grade blades. (e1+e2)Çe1 = 0 while (e1+e2)Çe1 = e1 . 

    \section{Precedence Conventions}

In much of the GA literature, precendence conventions are adopted for multivector products with outer 
products taking precedence over inner products, which in turn take precedence over geometric products so 
that, for example, $\mathbf{a\cdot bc \wedge d\cdot e}$ denotes $\mathbf{(a\cdot b)((c\wedge d)\cdot e)}$.
We will not rely on such conventions, favouring explicit brackets with regard to product symbol scope 
throughout this work, in acordance with programmer morality.

    
    \chapter{Multivector Operations}

    \section{Lifts}
    \section{Conjugations}
    \section{Scalar Measures and Normalising}
    \section{Inverses and Powers}

    \subsection{Inverse}
    \subsection{Integer Powers}
    \subsection{Square roots}
    
    \section{Exponentials and Logarithms}

    \subsection{Introduction}
    \subsection{Exponential}
    \subsubsection{Baker-Campbell-Hausdorf formulae}
    \subsubsection{Exponentiating Products}
    \subsubsection{Exponentiating Idempotents}
    \subsubsection{Exponentiating Annihilators}
    
    \subsection{Exterior Exponential}
    \subsection{Logarithm}

    \subsection{Hyperboloc Functions}
    \subsection{Central Powers}
    \subsection{Complex Numbers}
    \subsection{Hyperbolic Numbers}
    \subsection{Nullic Numbers}
    \subsection{Bi-Imaginary Numbers}
    \subsection{Computing Exponentials and Logarithms}
    \subsection{Logarithm of Bivector Exponentiation}
    

    \section{Projections and Perpendiculars}
    \subsection{Projection}
    \subsection{Rejection}
    \subsection{Projection via anticommution}
    \subsection{Scaled Projections}
    \subsection{Normalised Projections}
    \subsection{Orthogonal Frames}
    
    
    \section{Intersections and Unions}
    \subsection{Join}
    \subsection{Meet}
    \subsection{Union}
    \subsection{Disjoint}
    \subsection{Plunge}
    \subsection{Null Blades}
    
    \section{Multivectors expressed as summed commuters}
    \subsection{ Bivectors expressed as sum of commuting 2-blades}

    
    
    \chapter{Multivectors as Geometric Objects}

\epigraph{As long as algebra and geometry have been separated, their progress have been slow and their uses limited; but when these two sciences have been united, they have lent each mutual forces, and have marched together towards perfection.}{\textit{Joseph-Louis Lagrange }}

    \section{Introduction}

     We have covered multivectors and their associated mathematical operations in some considerable depth. Now we turn at last to what they are good for. 

    \section{Subspaces}

We can regard a proper $k$-blade $\mathbf{a}_k$ as representing the set of 1-vectors 
$\{ \mathbf{x} : \mathbf{x}\wedge\mathbf{a}_k = 0 \}$ or, equivalently, 
$\{ \mathbf{x} : \mathbf{x}\lrcorner\mathbf{a}_k = 0 \}$ where $\mathbf{x}$
is understood to be a 1-vector from a vector space $V^n$ 
so that ak 1-represents the "space" of all 1-vectors (interpreted as points) satisfying a 
particular geometric condition.

\vspace{\baselineskip}

Since $\mathbf{x}\wedge\mathbf{a}_k$ is a pure $(k+1)-$blade the equivalence with 
zero condition involves $nC_{k+1}$ coordinates. More generally we can regard 
nonnull $\mathbf{a}_k$ as spanning the set of multivectors 
$\{ \mathbf{x} : \downarrow_{\mathbf{a}_k}(\mathbf{x})=\mathbf{x} \} = \{ \mathbf{x} : 
\mathbf{x}\lrcorner \mathbf{a}_k=\mathbf{x}\mathbf{a}_k \}$. 

\subsubsection{The zero blade}

From this perspective, the zero blade $\mathbf{0}$ represents $V^n$, 
as does the pseudoscalar $i$. This is unsatisfactory partly because 0 should not 
imply a particular dimension $n$. Bouma \cite{bouma2001} suggests interpreting 0 
instead as an indeterminate subspace. This is appropriate in a 
programming context, since an absolute magnitude measure of a blade 
then becomes a measure of its "ambiguity", with large values 
indicating robust exact representations, small values indicating 
possibly inexact results, and zero indicating a total absence of 
subspace identification rather than a particular subspace. 

\subsection{Lines and Planes}

We now generalise the concepts of lines and planes. Programmers traditionally represent 3D 
planes with a normal 1-vector and either a scalar distance (aka. directance) from a given origin 
point, or a particular "base" point in the plane. This approach fails to extend to higher 
dimensions where there is no unique normal 1-vector; rather a normal (dual) $(n-k)-$blade.

\vspace{\baselineskip}

A k-plane (aka a displaced subspace or a flat) is a set of the form { r : (r-a)Ùbk = 0 } =
{ r : rÙbk = aÙbk } where bk is a k-blade. In particular, a lies in the k-plane. An (N-1)-plane
is known as a hyperplane. 

\subsection{Simplexes}

Given a frame of k+1 vectors (a0,a1,a2,...ak) where k<N, their k-simplex is the convex hull defined by the points. The 1-simplex for (a0,a1) , for example, is the line segment connecting a0 to a1. The 2-simplex for (a0,a1,a2) is the flat triangular "surface element" defined by them.
     Writing ak º (a1-a0)Ù(a2-a0)Ù...Ù(ak-a0) ; where a0 is known as the base point, we note that the simplex is contained within the k-plane ak + a0Ùak which we will refer to as the extended k-simplex.
      ak + a0Ùak has moment a0Ùa1Ù...Ùak = a0Ùak which is nonzero only if all k+1 vectors are linearly independant.
    The scalar k!-1 |ak| is known as the content of the k-simplex; where |ak| º |ak2|½ is welldefined for any pureblade ak. For k=1 this is the length |a1-a0|; for k=2 it is the area ½|(a2-a0)Ù(a1-a0)|.

    Simplexes provide the basic construction element for the multidimensional integration operations of Geoemetric calculus.

    One of the fundamental limitations of multivectors is that while they can naturally be used to concisely represent and manipulate extended k-simplexes (the plane containing a given three points for example) and, as we will see below, spherical surfaces , with simple blades, they do not provide so ready a means of directly representing and manuipulating bounded k-simplexes such as as the triangular "facet" defined by a particular three points or the line "segment" defined by two. From a programming perspective, this is matter of some frustation. 

\subsection{Frames}

The obvious way to represent a $k$-frame $\{\mathbf{e}_1, \cdots ,\mathbf{e}_k\}$ 
is as $k$ 1-vectors, typically as the columns of a $n\times k$ real matrix. 
However alternate seldom discussed possibilities are the \emph{mixed multivectors} 
$\mathbf{A}_k = \mathbf{A}_1 + \mathbf{A_1 \wedge A_2} + \cdots + \mathbf{A_1\wedge A_2 \cdots \wedge A_n}$, or $\mathbf{A}_1 + \mathbf{A_1 A_2} + \cdots + \mathbf{A_1A_2\cdots A_k}$.
If we allow the grade selector operator ($\langle \rangle$), ie. taking only the 1-vector component, then
we can clearly recover $\mathbf{A}_1 = \langle \mathbf{A}_k \rangle_1$, and hence 
$\mathbf{A}_1=\mathbf{A_1}/(\mathbf{A_1^2})$. $\mathbf{A}_2$ 
is then available as $\mathbf{A_1 \lrcorner A_k}$, 
$\mathbf{A}_3$ as $\mathbf{A}_2\lrcorner(\mathbf{A}_1\lrcorner\mathbf{A}_k)$, and so forth.

\vspace{\baselineskip}

Suppose we transform the frame as ai'ºBaiB#-1 where B#-1B = b .
BakB#-1 is not the correct representor for the transformed frame, (BakB#-1)<i> requiring scaling by b1-i . However, provided $b>0$, we can unambiguously renormalise the $\mathbf{A}_i$ while reconstructing them from BakB#-1 so it is a representor for the transformed frame.


    
    \section{Higher Dimensional Embeddings}

We can think of $k$-blades as representing $k$-dimensional subspaces in $\mathbb{R}^n$.
For $n=3$, these correspond to lines and planes through the origin. We would like to be able to 
use the operators $(\downarrow,\bot,\cup,\cap)$ to manipulate general displaced subspaces of 
$\mathbb{R}^n$ (ie. ones not containing the origin). 
One way of doing this is to "embedd" $\mathbb{R}^n$ points in a 
higher dimensional space via a function $f : \mathbb{R}^n \rightarrow \mathbb{R}^{N+p,q,r}$
and work with the multivectors of that space. With each point $\mathbf{x}$ in
$\mathcal{R}_n$ we associate a point $\mathbf{x} = f(\mathbf{x})$ 
in the higher space. With each multivector in $\mathcal{R}_n$ we 
associate the "same" multivector with regard to an extended basis.

\textcolor{gray}{This is the idea behind the ´Grassmanian´ space
$\text{Gr}(n)$, which is the same as the ´multivector space´ or ´Universal Clifford Algebra´
associated to $\mathcal{R}_n$. That is, we look at the sums of all the linear subspaces
of a larger $\mathbb{R}^n$.}

Multivectors in the higher space are associated with particular structures in 
$\mathcal{R}_n$ and we can investigate these structures by working with the multivectors.
We look here at three popular embeddings. 
    
    \section{Homogeneous coordinates}

We move from $\mathcal{R}_n$ to $\mathcal{R}_{n+1}$ 
by the incorportaion of a new basis vector $\mathbf{e_0}$ 
with $\text{Sig}(\mathbf{e_0})=1$. The embedding is trivial: 

\begin{align*}
    f(\mathbf{x}) = \mathbf{x} + \mathbf{e_0}
\end{align*}

Some authors favour $\mathbf{x} \rightarrow \mathbf{e_0 x}$, mapping 1-vectors 
to 2-blades (when $\mathbf{e_0 \lrcorner x} = 0$), but we do not take this approach here.
For $n=3$, our multivectors now require 16 coordinates.

\vspace{\baselineskip}

Homogeneous coordinates are so called because they "desingularise" the origin point 0. 
In a Euclidean space $\mathbb{E}^n$, all points save 0 have a geometric inverse. 
By displacing all points by 1-vector $\mathbf{e_0}$ 
outside $\mathcal{R}_n$ we ensure that all points are invertible.

\textcolor{gray}{This framework is effectively the beginning of \emph{projective geometric
algebra} by introducing a nilpotent element that guarantees full invertibility; the mapping
of all vectors by a constant shift therefore includes now the inverse of 0, which is 
$\mathbf{e_0}$, the point ´at infinity´.}


    \subsection{$k$-planes}

Consider the ÂN+1 (k+1)-blade (a0+e0)Ùak where ak is tangent to the ÂN k-simplex for frame { a0,a1,..ak}.
(x+e0) Î (a0+e0)Ùak Û (x+e0)Ù(a0+e0)Ùak = 0 Û xÙa0Ùak = e0(x-a0)Ùak
Û (x-a0)Ùak = 0 which is the condition for membership in the k-plane containing the k-simplex.
Hence the k-simplex of any k+1 points in ÂN corresponds to a (k+1)-blade in ÂN+1. The simplex frame is not uniquely recoverable from the blade, only the extended simplex and measure of the content. In particular, the ÂN k-plane (1+x)Ùak is represented by ÂN+1 (k+1)-blade (e0+x)Ùak .
For example: 

\begin{itemize}
    \item The $\mathbb{R}^n$ point (0-plane) $\mathbf{p}$ corresponds to the
    $\mathcal{R}_{n+1}$ vector $\mathbf{p+e_0}$

    \item The $\mathbb{R}^n$ line (1-plane) through 2 distinct points $\mathbf{p,q}$, corresponds
    to the $\mathcal{R}_{n+1}$ 2-blade $\mathbf{(e_0 + p )\wedge (p-q)=(e_0+p)(e_0 + q)=
    \cup(e_0+p, e_0 + q)}$. Any segment of the same length of the extended line through $\mathbf{p}$ and
    $\mathbf{q}$ will generate this same $\mathcal{R}_{n+1}$ bivector.

    \item 
\end{itemize}
    
    \section{Affine Model}
    \section{Generalised Homogeneous Coordinates}

    \subsection{$\mathbf{e_0}$ and $\mathbf{e_\infty}$}

    \subsection{Geometric Interpretation Preview}

    \subsection{The Horosphere Point Embedding}

    \subsubsection{Inverse Point Embedding}
    \subsubsection{Embedded Products}
    
    \section{$k$-planes}
    \section{$k$-spheres}
    \section{Geometric Interpretation of GHC Blades}
    \section{Nonflat Embeddings}
    \section{$k$-conics}
    \section{Regeneralised Homogeneous Coordinates}
    \section{ The "One Up" Embeddings}
    \section{"Projective" GHC SubAlgebra}
    \section{Spherical Conformal Coordinates}
    \section{Tspherical Conformal Coordinates}
    \section{Soft Geometry}



    
    \chapter{Multivectors as Transformations}

\epigraph{It is a profoundly erroneous truism, repeated by all copy books and by eminent people when they are making speeches, that we should cultivate the habit of thinking of what we are doing. The precise opposite is the case. Civilization advances by extending the number of important operations which we can perform without thinking about them.}{\textit{ Alfred North Whitehead }}

Multivectors constitute an attractive alternative to the conventional matrix representations of transformations in $\mathbb{R}^n$. We will use uppercase mulitivector labels when we wish to emphasise a multivector's use in this role. 

    \section{Bivector Transform}

$\mathbf{f(a)}\equiv \mathbf{a} \lrcorner \mathbf{b}$ maps one 1-vector 
$\mathbf{a}$ into 2-vector $\mathbf{b}$ and rotates by $\pi/2$ 'within'
$\mathbf{b}$. For pure 2-blade $\mathbf{b}=\mathbf{e}_{12}$ we have
$\mathbf{f}(\Sigma_i a^i \mathbf{e}_i) = -\mathbf{a}^2\mathbf{e}_1+\mathbf{a}^1
\mathbf{e}_2$, it provides 1-vector transformations:

\begin{align*}
    \mathbf{f^k(a)} \equiv \mathbf{f^{k-1}(a)}\lrcorner\mathbf{b} = 
    2^{-k}\sum_{i=0}^k (-1)^k \mathbf{C}_i \mathbf{b}^i \mathbf{a} \mathbf{b}^{k-i}
\end{align*}

    \section{Lorentz Transforms}


    \section{Higher Dimensional Embeddings}

    \subsection{Homogeneous Coordinates}
    \subsection{Affine Model}
    \subsection{Generalised Homogeneous Coordinates}
    
    
    \section{Translations}

Translations in $\mathbb{R}^n$ can only be represented in 
$\mathbb{R}^n$ by addition of a 1-vector in the conventional manner, eg.
$\mathbf{x+d}$.

\vspace{\baselineskip}
    
In some higher dimensional emebeddings we can exploit the fact that a 
translation by displacement vector $\mathbf{d}$ is equivalent to
reflection in any two hyperplanes normal to $\mathbf{d}$ 
spaced $|\mathbf{d}|/2$ apart. In particular, the
$\mathbb{R}^n$ hyperplanes through $\mathbf{O}$ 
and then $\mathbf{d}/2$ will suffice.

\subsection{Affine model}

The representor of $\mathbf{x-d}$ is:
\begin{align*}
    1+\mathbf{e}_0(\mathbf{x-d})=\mathbf{T_d}(1+\mathbf{e}_0\mathbf{x})
    \mathbf{T_d}\\
    \mathbf{T_d}=\left ( 1-\frac{1}{2}\mathbf{e}_0\mathbf{d} \right )=
    \left ( - \frac{1}{2} \mathbf{e_0d} \right )^{\uparrow}
\end{align*}

The algebra of $\mathbf{T_d}$ implies:

\begin{itemize}
    \item $\mathbf{T_d}^2 = \mathbf{T_{2d}}$
    \item $\mathbf{T_d}^{#}\mathbf{T_d}=\tilde{\mathbf{T_d}}\mathbf{T_d}$
    \item $\mathbf{T_d}^{-1}$
\end{itemize}

If $\mathbf{A}$ is any multivector representing an 'origin centered'
transformation of $\mathbb{R}^n$ we can represent the associated
operation centered at $\mathbf{d}$ with $\mathbf{T_{-d}}\mathbf{A}
\mathbf{T_d}$. If $\mathbf{A}$ is even we have:

\begin{align*}
    \mathbf{T_{-d}}\mathbf{A}\mathbf{T_d} &=
    \left ( 1+\frac{1}{2}\mathbf{e}_0\mathbf{d} \right )
    \mathbf{A}
    \left ( 1-\frac{1}{2}\mathbf{e}_0\mathbf{d} \right ) \\
    &=
    \mathbf{A} - \frac{1}{2} \left ( \mathbf{Ae_0 d}-
    \mathbf{e_0dA} \right ) \\
    &=
    \mathbf{A} + \frac{1}{2} (\mathbf{dA-Ad}) \\
    &= \mathbf{A} + \frac{1}{2} \mathbf{e_0}(\mathbf{d\times A})
\end{align*}

\subsection{Generalised Homogeneous Coordinates}




    \section{Reflections}
    \section{Shears and Strains}
    \section{Rotations}
    \section{Inversion}
    \section{Dilation and Involution}
    \section{Perspective Projection}
    \section{Summary of GHC Transformations}

\begin{table}[h]
\centering
\begin{tabular}{|c|c|c|}
\hline
\cellcolor[HTML]{CBCEFB}Transform $[\mathbf{A}^\bot \mathbf{(x)}=\mathbf{AxA^\top}]$ &
  \cellcolor[HTML]{BCA3B7}Effect &
  \cellcolor[HTML]{FFFC9E}$\sigma(x)s$ \\ \hline
\rowcolor[HTML]{FFFFFF} 
$\left ( \frac{1}{2}\mathbf{e}_{\infty}\mathbf{d}\right )^\uparrow$ &
  Translation by $\mathbf{d}\in i$ &
  1 \\ \hline
\rowcolor[HTML]{EFEFEF} 
$\left ( \frac{1}{2}\mathbf{e}_0\mathbf{d}\right )^\uparrow$ &
  Transversion by $\mathbf{d}\in i$ &
  1 \\ \hline
\rowcolor[HTML]{FFFFFF} 
$\left (- \frac{1}{2}\mathbf{e}_{\infty0} \right)^\uparrow$ &
  Dilation (scaling) by $\phi^\uparrow$ &
  $(-\phi)^{\uparrow}$ \\ \hline
\rowcolor[HTML]{EFEFEF} 
\cellcolor[HTML]{EFEFEF}$\left ( \mathbf{e}_{\infty0} \left (  -\frac{1}{2}\phi \mathbf{e}_{\infty 0} \right )^\uparrow  \right )$ &
  Dilation (scaling) by $-\phi^\uparrow$ &
  $-(-\phi)^{\uparrow}$ \\ \hline
\rowcolor[HTML]{FFFFFF} 
$\left ( \frac{1}{2} \theta(\mathbf{a\wedge n}) \right )^\uparrow$ &
  Minimal rotation taking $\mathbf{n}$ to $\mathbf{a}$ &
  1 \\ \hline
\rowcolor[HTML]{EFEFEF} 
AA &
  Rotation of $\theta$ about axis $\mathbf{c}+\lambda\mathbf{a}$, where $\mathbf{a}^2=1$ &
  1 \\ \hline
\rowcolor[HTML]{FFFFFF} 
$\tilde{R}$ &
  Rotation by even rotor $R$ wtih $R\tilde{R}=1$ &
  1 \\ \hline
\rowcolor[HTML]{EFEFEF} 
$-(\mathbf{A}_{k+2})_{\#-1}$ &
  Reflection in $k$-sphere or $k$-plane $\mathbf{A}_{k+2}$ &
  -1 \\ \hline
\end{tabular}
\caption{The full set of transformations in projective (GHC) coordinates, utilizing the nilpotent elements of projective geometric algebra. These are very useful in computer graphics.}
\label{tab:my-table}
\end{table}
    
    \section{Spinor Transforms}
    

    
    \chapter{Multivector Mathematical Arcana}

    \section{Algebraic Equivalences}
    
    \epigraph{Analytical geometry has never existed. There are only people who do linear geometry badly, by taking coordinates, and they call this analytical geometry. Out with them!}{\textit{Jean Dieudonné }}

    \subsection{Relativity of Signatures and Grades}

    \subsection{Algebraic Product Formulations}

    \subsection{Complexification}

    \section{Minimal Geometric Algebra}

Our geometric vocabulary is somewhat broad with various product symbols $(\lrcorner, wedge, \cdots)$,
operators $(+,/,↑,\cdots)$, conjugations $(§,\tilde,\cdots)$, and so forth. 
Which symbols are fundamental in that they cannot be semantically defined 
using other fundamental symbols?

\vspace{\baselineskip}
    
Let us first suppose just $+$ and $\otimes$, precedence indication brackets $(/)$, an 
orthonormal $n$-frame symbol set ${\mathbf{e}_1,\mathbf{e}_2,\cdots \mathbf{e}_n}$
and some multivector "variable" symbols $\mathbf{A}, \mathbf{B}, \cdots$
together with an assigment statement $=$.

\vspace{\baselineskip}

Unity symbols $\mathbf{1}$ and $\mathbf{-1}$ (and hence the left and right negation conjugation operators $(_)$ and $(-)$ ) are available from 
${\mathbf{e}_1\mathbf{e}_1, \mathbf{e}_2\mathbf{e}_2, \mathbf{e}_{12}\mathbf{e}_{12}}$ depending on the signatures and we obtain, semantically at least, all the integer-coordinated multivectors in the given frame. 

\vspace{\baselineskip}

Exponentiation ↑ can be defined using + and ¨ given some form of infinitite summation å symbolism and if we then suppose logarithm ↓ as a particular inverse of ↑ we obtain a-1 as (-(a↓))↑ which we can write as -1 = ↓-↑ whence a/b º a(b-1)   =   a(b↓-↑) and we attain the general real-coordinate multivector space. [under construction] 

    \section{Bivectors}

    \subsection{Expressed as sum of commuting 2-blades}

    \section{Spinors}

    

    \subsection{Spinor Adjustement Rules}

Whenever $i^2=-1$ and commutes with $\mathbf{A}$ 
we have the \emph{\textbf{spinor adjustment rule}}:

\begin{tcolorbox}[colback=white, colframe=blue!10!black, title=\textbf{Spinor
Adjustement Rules} ]

\begin{align*}
    &(1+\mathbf{A})e^{i\phi} = (1+\mathbf{A})e^{i\mathbf{A}\phi} =
    (1+\mathbf{A})e^{-\mathbf{A}^* \phi}, \quad \mathbf{A}^2=1
\\
    &(1-\mathbf{A})e^{i\phi} = (1-\mathbf{A})e^{-i\mathbf{A}\phi} =
    (1+\mathbf{A})e^{-\mathbf{A}^* \phi}, \quad \mathbf{A}^2=-1 
\end{align*}

\end{tcolorbox} 

A proof is made by expanding $e^{i\phi}$ trigonometrically:

\begin{align*}
    0&=(1+\mathbf{A})(e^{i\phi}-e^{i\mathbf{A}\phi})\\
    &=(1+\mathbf{A})\sin(\theta)(i-i\mathbf{A}) \\
    &=(1+\mathbf{A})(1-\mathbf{A})i\sin(\theta), \quad
    \mathbf{A}^2=1
\end{align*}

In $\mathbb{R}^3$ we have $(1+\mathbf{A})e^{i\phi}=(1+\mathbf{A})e^{i\mathbf{A}\phi}$,
which means that under a $(1+\mathbf{A})$ multiplier we can replace $\mathbf{A}i$
exponentiation by $i$ exponentiation, which is generally more commutative and therefore
easier to do.

    \subsection{Alternate representations of spinors}

Physicists tend to represent spinors in convoluted and confusing ways that obsfucate more
than they reveal \textcolor{gray}{(Tell me about it!)}. 
In Quantum Mechanics, a 'spinor' is typically defined via two complex
numbers $\mathbf{z_1} = a_0+ib_0 = r_0 e^{i\phi_0}$ and 
$\mathbf{z_2} = a_1+ib_1 = r_1 e^{i\phi_1}$. For a spinor, 
$\mathbf{z_2} = re^{i\phi}r_0e^{i\phi_1}$; when $r_0 \neq 0$ as the singular
(zero determinant) $2\times 2$ complex matrix:

\begin{align*}
    \mathbf{s}
&=
\begin{pmatrix}
  zw& -w^2 \\
  w^2& -z
\end{pmatrix}
=
\begin{pmatrix}
  (a_0+ib_0)(a_1+ib_1)&-(a_1+ib_1)^2 \\
  (a_1+ib_1)^2&-(a_0+ib_0)
\end{pmatrix}
\\
\\
&=
\begin{pmatrix}
 r_0r_1e^{i(\phi_0+\phi_1)}  &  r_0^2 e^{i2\phi_0} \\
  r_1^2 e^{i2\phi_1}  &  -r_0r_1e^{i(\phi_0+\phi_1)}
\end{pmatrix}
=
r_0^2 e^{i2\phi_0}
\begin{pmatrix}
 re^{i\phi}  &  -1 \\
  r^2e^{i2\phi}  &  -re^{i\phi}
\end{pmatrix}
\end{align*}

The spinor is considered \emph{normalized} if its real scalar part 
$(1/2)(a_0^2+b_0^2+a_1^2+b_1^2)=(1/2)(r_0^2+r_1^2)$ of the singular matrix is equal
to $1/2$.


    \subsection{Riemann Sphere Representations}

 In Riemann sphere representation of $\mathbf{C}+\infty$ we map complex "points" $z=a+bi=re^{i\phi}$
 to the real 3D unit 1-vector corresponding to the intersection of the line joining a $\mathbb{R}^3$ point
 $a\mathbf{e}_1+b\mathbf{e}_2$ to the "south pole" $-\mathbf{e}_3$ with the unit sphere centred at 0. 
 This point is given in spherical polar coordinates as:

\begin{tcolorbox}[colback=white, colframe=blue!40!black, title=\textbf{Riemman projection rules} ]

 \begin{align}
     \text{Riemann}(\mathbf{z}) &= (2 \arctan(r), \phi, 1) \\ 
     &= \exp(\phi\mathbf{e}_{12})\exp(-2 \arctan(r)\mathbf{e}_{31})\mathbf{e}_3 \\
     &= (1+r^2)^{-1}( \mathbf{e}_3(1-r^2) + 2r(\cos \phi \mathbf{e}_1 + \sin \phi \mathbf{e}_2)
    \end{align}

We further define:

\begin{itemize}
    \item $\text{Riemann}(0) = -\mathbf{e}_3$
    \item $\text{Riemann}(\mathbf{z}^{-1}) = (\pi-2 \arctan(r), -\phi, 1)$
    \item $\text{Riemann}(-\mathbf{z}) = (2 \arctan(r), \phi+\pi,1)$
    \item $\text{Riemann}(e^{i\phi}) = \mathbf{e}_1 e^{\phi \mathbf{e}_{12}} $
\end{itemize}

If we site the complex plane tangent at the "north pole" $\mathbf{e}_3$
rather that at the equator we have $\text{Riemman}(\mathbf{z}) = ( 2 \arctan(r/2), \phi,1)$.  

\end{tcolorbox} 

A proof of the last statement:

\begin{align*}
    \mathbf{u}^2 &= (r \cos(2\phi)-1)^2+r^4 \sin^2(2\phi) + 4r^2\cos^2(\phi) \\
    &=
\end{align*}


\section{Multivector Kinematics}

A natural way to represent a 3D 'oriented point' conventionally represented by $\mathbf{c}$, 
$A=(i,j,k)$ for an orthogonal $3\times3$ real matrix $A$ in GHC space $\mathbb{R}^{4,1}$ 
is with the unit plussquare 3-blade $(\mathbf{c}-\mathbf{e}_{\infty}/2)\wedge \mathbf{k}^*a_3$
representing a unit radius 1-sphere (circle) of centre $\mathbf{c}$, and axis $\mathbf{k}$ 
scaled in some manner by a measure of Eulerian angle $\phi$. 

\vspace{\baselineskip}

We define \emph{matrix-multivector} products by treating matrix columns as separate 1-vectors:

\begin{itemize}
    \item $A(a_1,a_2, \cdots) = (Aa_1,Aa_2, \cdots)$
    \item $(a_1,a_2, \cdots)A = (a_1A,a_2A, \cdots)$
\end{itemize}

 All other products ($\lrcorner, \wedge, \cdots$) are treated likewise.


\subsection{Rigid Body Kinematics}

Let $\mathbf{H}$ be an $n$-D object moving through $V^n$ space and rotating about a 
central point $\mathbf{G}$ which traverse a path $\mathbf{x}_0(t)$. Let 
$A(t)$ be the orientation matrix of the object at time $t$.

\vspace{\baselineskip}

We can represent the rotation with a unit rotor $R(t)$ (satisfying $R(t)\tilde{R(t)} = 1$) as:

\begin{align*}
$A(t) = \tilde{R(t)}A(0) = R(t)A(0)\tilde{R(t)}$
\end{align*}

For brevity we now drop the explicit $t$, write 0 for (0), and let $'$ denote differentiation with respect to $t$. We then have $A = RA_0\tilde{R}$. 

\vspace{\baselineskip}




These \emph{\textbf{rotor equations}} are generally easier to solve than their matrix counterparts. An aesthetic advantage in defining angular velocities and momenta as bivectors rather than vectors is the avoidance of invoking a third dimension to "hold" them in the case of otherwise planar kinematics.  


\bibliographystyle{abbrv}
\bibliography{referencias}

    
    

\end{document}